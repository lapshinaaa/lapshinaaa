{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapshinaaa/lapshinaaa/blob/main/RecSys4_NeuralRanking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWT4fY6Hd7I3"
      },
      "source": [
        "# Neural Ranking\n",
        "\n",
        "Source: https://github.com/yandexdataschool/recsys_course/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii9YR_DMSC9H"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "heGGt6crwhpA"
      },
      "outputs": [],
      "source": [
        "import typing as tp\n",
        "import polars as pl\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F8ykYo-eg9X"
      },
      "source": [
        "## 1. Downsampled Criteo Kaggle Dataset\n",
        "\n",
        "Source:\n",
        "https://www.kaggle.com/datasets/dogrose/downsampled-criteo-kaggle-dataset/data\n",
        "\n",
        "The Criteo Dataset is a large-scale benchmark for Click-Through Rate (CTR) prediction in online advertising. It contains real ad impression logs and is widely used for training and evaluating models that estimate the probability of a user clicking on an advertisement.\n",
        "\n",
        "Dataset Characteristics\n",
        "Feature Type | Count | Description\n",
        "------------|-------|------------\n",
        "Target | 1 | Binary click indicator (clicked / not clicked)\n",
        "Numerical Features | 13 (I1–I13) | Count-based, require log transformation\n",
        "Categorical Features | 26 (C1–C26) | Anonymized, hashed, high cardinality\n",
        "\n",
        "Additional properties:\n",
        "- Contains millions of samples across 7 days (6-day train, 1-day test)\n",
        "- High sparsity and many missing values\n",
        "- The Downsampled version used here is 10× smaller than the original dataset\n",
        "\n",
        "The original dataset requires substantial preprocessing; this is implemented in the provided code.\n",
        "\n",
        "Preprocessing Techniques Used in Literature\n",
        "- Log transformation of numerical features\n",
        "- Count encoding or one-hot encoding for categorical features\n",
        "- Feature hashing for high-cardinality variables\n",
        "- Feature interactions (manual or learned)\n",
        "\n",
        "Modeling Approaches Used by Top Kaggle Solutions\n",
        "Category | Example Models | Notes\n",
        "---------|---------------|------\n",
        "Gradient Boosting | XGBoost, LightGBM, CatBoost | Strong baselines\n",
        "Deep Learning | Wide & Deep, DNNs | Capture non-linear interactions\n",
        "Factorization Machines | FM, FFM | Good for sparse categorical data\n",
        "Ensembles | Stacking multiple models | Often leads to top performance\n",
        "\n",
        "Top solutions combine multiple categories and emphasize feature engineering + handling high cardinality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_l8oSvuxFSP",
        "outputId": "75bed2f6-5036-4329-f486-119702614ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  208M  100  208M    0     0   149M      0  0:00:01  0:00:01 --:--:--  233M\n",
            "Archive:  ./data/downsampled-criteo-kaggle-dataset.zip\n",
            "  inflating: ./data/criteo_test_1day_downsampled.parquet  \n",
            "  inflating: ./data/criteo_train_6days_downsampled.parquet  \n"
          ]
        }
      ],
      "source": [
        "!mkdir ./data\n",
        "!curl -L -o ./data/downsampled-criteo-kaggle-dataset.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/dogrose/downsampled-criteo-kaggle-dataset\n",
        "!unzip ./data/downsampled-criteo-kaggle-dataset.zip -d ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7b2Md-m-2C_u"
      },
      "outputs": [],
      "source": [
        "class CriteoDatasetUtils:\n",
        "    INT_COLS = [f'I{i + 1}' for i in range(13)]\n",
        "    CAT_COLS = [f'C{i + 1}' for i in range(26)]\n",
        "    LABEL_COl = 'label'\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess_dense_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Preprocess dense features:\n",
        "        - Fill missing values with 0\n",
        "        - Apply log transformation: log(x+1) or log(x+4)\n",
        "        \"\"\"\n",
        "        expressions = []\n",
        "        for col in cls.INT_COLS:\n",
        "            expressions.append(\n",
        "                pl.col(col).fill_null(0).add(1 if col != 'I2' else 4).log()\n",
        "            )\n",
        "        lf = lf.with_columns(expressions)\n",
        "        return lf\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess_categorical_features(cls, lf: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Preprocess categorical features:\n",
        "        - Fill missing values with zero string (\"00000000\")\n",
        "        - Convert from hex to Int64\n",
        "        \"\"\"\n",
        "        expressions = []\n",
        "        for col in cls.CAT_COLS:\n",
        "            expressions.append(\n",
        "                pl.col(col).fill_null(\"00000000\").str.to_integer(base=16)\n",
        "            )\n",
        "        lf = lf.with_columns(expressions)\n",
        "        return lf\n",
        "\n",
        "    @classmethod\n",
        "    def read_and_preprocess(cls, path: str) -> pl.DataFrame:\n",
        "        lf = pl.scan_parquet(path)\n",
        "        lf = cls.preprocess_categorical_features(lf)\n",
        "        lf = cls.preprocess_dense_features(lf)\n",
        "        return lf.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q0oZv1MNsHqh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "24d95c79-35ff-4c34-e93a-d802aaf98805"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 40)\n",
              "┌───────┬──────────┬──────────┬──────────┬───┬───────────┬────────────┬────────────┬────────────┐\n",
              "│ label ┆ I1       ┆ I2       ┆ I3       ┆ … ┆ C23       ┆ C24        ┆ C25        ┆ C26        │\n",
              "│ ---   ┆ ---      ┆ ---      ┆ ---      ┆   ┆ ---       ┆ ---        ┆ ---        ┆ ---        │\n",
              "│ i64   ┆ f64      ┆ f64      ┆ f64      ┆   ┆ i64       ┆ i64        ┆ i64        ┆ i64        │\n",
              "╞═══════╪══════════╪══════════╪══════════╪═══╪═══════════╪════════════╪════════════╪════════════╡\n",
              "│ 0     ┆ 0.693147 ┆ 1.609438 ┆ 1.791759 ┆ … ┆ 974593739 ┆ 3318023300 ┆ 3904386055 ┆ 2535972118 │\n",
              "│ 0     ┆ 0.0      ┆ 1.791759 ┆ 6.45047  ┆ … ┆ 974593739 ┆ 3470446969 ┆ 3935970412 ┆ 2589289724 │\n",
              "│ 1     ┆ 0.0      ┆ 3.931826 ┆ 0.0      ┆ … ┆ 851920782 ┆ 3829933919 ┆ 0          ┆ 0          │\n",
              "│ 1     ┆ 0.0      ┆ 5.638355 ┆ 0.0      ┆ … ┆ 974593739 ┆ 2427856751 ┆ 0          ┆ 0          │\n",
              "│ 0     ┆ 0.0      ┆ 3.73767  ┆ 1.098612 ┆ … ┆ 851920782 ┆ 991444060  ┆ 0          ┆ 0          │\n",
              "└───────┴──────────┴──────────┴──────────┴───┴───────────┴────────────┴────────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 40)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>label</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>I8</th><th>I9</th><th>I10</th><th>I11</th><th>I12</th><th>I13</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th>C10</th><th>C11</th><th>C12</th><th>C13</th><th>C14</th><th>C15</th><th>C16</th><th>C17</th><th>C18</th><th>C19</th><th>C20</th><th>C21</th><th>C22</th><th>C23</th><th>C24</th><th>C25</th><th>C26</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>0.693147</td><td>1.609438</td><td>1.791759</td><td>0.0</td><td>7.23201</td><td>1.609438</td><td>2.772589</td><td>1.098612</td><td>5.204007</td><td>0.693147</td><td>1.098612</td><td>0.0</td><td>1.098612</td><td>1761418852</td><td>2162322587</td><td>4220739894</td><td>2068259780</td><td>633879704</td><td>2114768079</td><td>3732510136</td><td>529118562</td><td>2805916944</td><td>2832028932</td><td>2999688344</td><td>935969124</td><td>673490422</td><td>450684655</td><td>2343089050</td><td>2300273383</td><td>3854202482</td><td>4114618041</td><td>568184265</td><td>2972002973</td><td>129309004</td><td>0</td><td>974593739</td><td>3318023300</td><td>3904386055</td><td>2535972118</td></tr><tr><td>0</td><td>0.0</td><td>1.791759</td><td>6.45047</td><td>0.0</td><td>10.946781</td><td>0.0</td><td>0.0</td><td>1.791759</td><td>4.189655</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.098612</td><td>98275684</td><td>73979506</td><td>2062028047</td><td>2161661274</td><td>633879704</td><td>2114768079</td><td>69696505</td><td>185940084</td><td>2093428418</td><td>990438539</td><td>2116836373</td><td>3486017542</td><td>2443294225</td><td>2995026422</td><td>1478826667</td><td>342520061</td><td>2003624857</td><td>187896596</td><td>568184265</td><td>1480633834</td><td>3421256155</td><td>0</td><td>974593739</td><td>3470446969</td><td>3935970412</td><td>2589289724</td></tr><tr><td>1</td><td>0.0</td><td>3.931826</td><td>0.0</td><td>0.0</td><td>8.764053</td><td>3.663562</td><td>2.995732</td><td>2.397895</td><td>4.969813</td><td>0.0</td><td>2.397895</td><td>0.0</td><td>1.94591</td><td>342162125</td><td>950618017</td><td>574295574</td><td>3394569756</td><td>633879704</td><td>2114768079</td><td>1765007041</td><td>1530472565</td><td>2805916944</td><td>990438539</td><td>2248945707</td><td>359635439</td><td>812864628</td><td>450684655</td><td>242755870</td><td>1606371972</td><td>3854202482</td><td>429505257</td><td>0</td><td>0</td><td>3736690781</td><td>0</td><td>851920782</td><td>3829933919</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0.0</td><td>5.638355</td><td>0.0</td><td>1.386294</td><td>8.898229</td><td>3.218876</td><td>1.94591</td><td>1.386294</td><td>4.59512</td><td>0.0</td><td>0.693147</td><td>0.0</td><td>1.386294</td><td>2364568165</td><td>2598325497</td><td>779549537</td><td>186309082</td><td>1291264903</td><td>4268462821</td><td>1977395914</td><td>185940084</td><td>2805916944</td><td>990438539</td><td>2326518504</td><td>2558959783</td><td>3989772238</td><td>131152527</td><td>716094755</td><td>864816393</td><td>3854202482</td><td>3353078997</td><td>0</td><td>0</td><td>1624430225</td><td>0</td><td>974593739</td><td>2427856751</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0.0</td><td>3.73767</td><td>1.098612</td><td>1.609438</td><td>8.045588</td><td>5.010635</td><td>4.174387</td><td>3.89182</td><td>4.941642</td><td>0.0</td><td>1.94591</td><td>1.94591</td><td>1.609438</td><td>98275684</td><td>648577312</td><td>3492987491</td><td>3247169921</td><td>1291264903</td><td>4222442646</td><td>1062127239</td><td>529118562</td><td>2805916944</td><td>1919877373</td><td>3299735832</td><td>3753576955</td><td>2245779768</td><td>131152527</td><td>68076599</td><td>2223606570</td><td>2399067775</td><td>1465486885</td><td>0</td><td>0</td><td>1360682</td><td>0</td><td>851920782</td><td>991444060</td><td>0</td><td>0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# DATASETS_PATH = '/content'\n",
        "DATASETS_PATH = './data'\n",
        "train_df = CriteoDatasetUtils.read_and_preprocess(f'{DATASETS_PATH}/criteo_train_6days_downsampled.parquet')\n",
        "test_df = CriteoDatasetUtils.read_and_preprocess(f'{DATASETS_PATH}/criteo_test_1day_downsampled.parquet')\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PTOXI_8JSC9L"
      },
      "outputs": [],
      "source": [
        "assert train_df.null_count().pipe(sum).item() == 0\n",
        "assert test_df.null_count().pipe(sum).item() == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPNdQnZxSC9L"
      },
      "source": [
        "Taking a look at the number of unique values of categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb3kTxiRSC9M",
        "outputId": "44b42a14-1588-405a-c538-395b7b585dff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C3': 1203747,\n",
              " 'C12': 1047892,\n",
              " 'C21': 925198,\n",
              " 'C16': 759151,\n",
              " 'C4': 378887,\n",
              " 'C24': 90428,\n",
              " 'C26': 59490,\n",
              " 'C10': 50127,\n",
              " 'C7': 11950,\n",
              " 'C15': 11775,\n",
              " 'C11': 5215,\n",
              " 'C18': 4756,\n",
              " 'C13': 3164,\n",
              " 'C19': 2036,\n",
              " 'C1': 1452,\n",
              " 'C8': 628,\n",
              " 'C2': 556,\n",
              " 'C5': 303,\n",
              " 'C25': 94,\n",
              " 'C14': 26,\n",
              " 'C6': 18,\n",
              " 'C22': 17,\n",
              " 'C23': 15,\n",
              " 'C17': 10,\n",
              " 'C20': 4,\n",
              " 'C9': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "unique_counts = {col: train_df[col].n_unique() for col in CriteoDatasetUtils.CAT_COLS}\n",
        "sorted_unique_counts = dict(\n",
        "    sorted(unique_counts.items(), key=lambda item: item[1], reverse=True)\n",
        ")\n",
        "sorted_unique_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6P3ipgTSC9M"
      },
      "source": [
        "How much GPU will be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdr0dJDsSC9M",
        "outputId": "1bac5838-b02b-434d-d3cf-d2ad65282198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.38335418701172 GB\n"
          ]
        }
      ],
      "source": [
        "uniq_ids = sum(sorted_unique_counts.values())\n",
        "embedding_dim = 256\n",
        "bytes_in_float = 4\n",
        "mult = 4 # params + grads + moment1 + moment2\n",
        "bytes_in_gb = 1024 * 1024 * 1024\n",
        "print(f'{uniq_ids * embedding_dim * bytes_in_float * mult / bytes_in_gb} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SsXfo6pvhvz"
      },
      "source": [
        "## 2. Multisize Unified Embeddings\n",
        "\n",
        "To encode categorical features, we will use Multisize Unified encoding from Google DeepMind: [Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems](https://arxiv.org/abs/2305.12102).\n",
        "\n",
        "### General task\n",
        "\n",
        "Given $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_{|D|}, y_{|D|})\\}$ containing samples with $T$ categorical features with vocabularies $\\{V_1, V_2, \\ldots, V_T\\}$. Each sample is of the form $x = [v_1, v_2, \\ldots, v_T]$, where $v_i \\in V_i$.\n",
        "\n",
        "- Embedding matrix $\\mathbf{E} \\in \\mathbb{R}^{M \\times d}$ maps a sample to an embedding $g(\\mathbf{x}; \\mathbf{E})$.\n",
        "- Hash function $h(v) : V \\rightarrow [M]$ assigns a feature value to a row index (used in $g(\\mathbf{x}; \\mathbf{E})$).\n",
        "- Model function $f(\\mathbf{e}; \\boldsymbol{\\theta})$ converts embeddings into predictions.\n",
        "\n",
        "Training objective:\n",
        "\n",
        "$$\\arg \\min_{\\mathbf{E}, \\boldsymbol{\\theta}} \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}), \\quad \\text{where} \\quad \\mathcal{L}_D(\\mathbf{E}, \\boldsymbol{\\theta}) = \\sum_{(\\mathbf{x},y) \\in D} \\ell(f(g(\\mathbf{x}; \\mathbf{E}); \\boldsymbol{\\theta}), y).$$\n",
        "\n",
        "We use $h_t(v)$ for each feature $t \\in [T]$. Let $\\mathbf{e}_m$ denote the $m$-th row of $\\mathbf{E}$ and $\\mathbb{1}_{u,v}$ be the indicator of hash collisions between $u$ and $v$.\n",
        "\n",
        "### How it works\n",
        "\n",
        "For simplicity, assume $|T| = 2$.\n",
        "\n",
        "<div style=\"width:90%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/GKMKcvm/unified-embeddings.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Caleb_SSC9N"
      },
      "source": [
        "### Why This Works (Intuition)\n",
        "\n",
        "Consider a special case: we are solving binary classification using logistic regression:\n",
        "\n",
        "$$y_i \\in \\{0, 1\\}$$  \n",
        "$$ D_0 = \\{(x_i, y_i) \\in D : y_i = 0\\} $$\n",
        "$$ D_1 = \\{(x_i, y_i) \\in D : y_i = 1\\} $$\n",
        "$$ C_{u,v,0} = |\\{([u, v], y) \\in D : y = 0\\}| $$\n",
        "$$ \\sigma_\\theta(z) = \\frac{1}{1 + \\exp(-\\langle z, \\theta \\rangle)} $$\n",
        "$$ z = g(x; \\mathbf{E}) = [e_{h_1(x_1)}, e_{h_2(x_2)}] $$\n",
        "$$ \\theta = [\\theta_1, \\theta_2],~\\theta_t \\in \\mathbb{R}^M $$\n",
        "\n",
        "Binary cross-entropy loss:\n",
        "\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{(x,y)\\in D_0} \\log \\left( \\frac{1}{1 + \\exp(-\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) - \\sum_{(x,y)\\in D_1} \\log \\left( \\frac{1}{1 + \\exp(\\langle \\theta, g(x; \\mathbf{E}) \\rangle)} \\right) $$\n",
        "\n",
        "Rewrite the loss function using co-occurrence frequencies:\n",
        "\n",
        "$$ e_{u,v} = [e_{h_1(u)}, e_{h_2(v)}] $$\n",
        "\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\left( \\frac{1}{1 + \\exp(\\theta^\\top e_{u,v})} \\right) + C_{u,v,1} \\log \\left( \\frac{1}{1 + \\exp(\\theta^\\top e_{u,v})} \\right) $$\n",
        "\n",
        "After combining sigmoid terms:\n",
        "\n",
        "$$ \\mathcal{L}_D(\\mathbf{E}, \\theta) = - \\sum_{u\\in V_1} \\sum_{v\\in V_2} C_{u,v,0} \\log \\exp(\\theta^\\top e_{u,v}) - (C_{u,v,0} + C_{u,v,1}) \\log(1 + \\exp(\\theta^\\top e_{u,v})) $$\n",
        "\n",
        "Now assume we train using SGD. Compute gradients with respect to embeddings. The full gradient, accounting for intra- and inter-feature interactions:\n",
        "\n",
        "$$ \\nabla_{E_{h(u)}} \\mathcal{L}_D(\\mathbf{E}, \\theta) = $$\n",
        "$$ \\theta_1 \\sum_{v\\in V_2} C_{u,v,0} - (C_{u,v,0} + C_{u,v,1})\\sigma_\\theta(e_{u,v}) \\tag{1}$$\n",
        "$$ + \\theta_1 \\sum_{w\\in V_1, w\\neq u} \\mathbb{1}_{u,w} \\sum_{v\\in V_2} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{u,v}) \\tag{2}$$\n",
        "$$ + \\theta_2 \\sum_{v\\in V_2} \\mathbb{1}_{u,v} \\sum_{w\\in V_1} C_{w,v,0} - (C_{w,v,0} + C_{w,v,1})\\sigma_\\theta(e_{w,u}) \\tag{3}$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- (1) collisionless component  \n",
        "- (2) intra-feature component  \n",
        "- (3) inter-feature component  \n",
        "- Components (2) and (3) bias the true gradient\n",
        "- Intra-feature bias aligns with the collisionless gradient, so the model *cannot* remove it\n",
        "- Under SGD, inter-feature bias can be mitigated if $\\theta_1 \\perp \\theta_2 $\n",
        "\n",
        "Reason: during SGD, $( e_{h(u)} $) is a linear combination of updates, meaning it decomposes into components along $\\theta_1$ and $\\theta_2$. Since:\n",
        "\n",
        " $$\\langle\\theta_1, \\theta_2\\rangle = 0$$\n",
        "\n",
        "the projection $\\theta_1^\\top e_{h(u)}$ filters out the inter-feature component.\n",
        "\n",
        "<div style=\"width:70%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/xt6nbrZ3/theory-unified.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Not all collisions are equally harmful.  \n",
        "- Inter-feature collisions can be mitigated by the model, since different features have distinct learned parameters.  \n",
        "- Intra-feature collisions are persistent and must be reduced via multiple hash functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nRvHPMaN9vrI"
      },
      "outputs": [],
      "source": [
        "class MultihashTransform:\n",
        "    \"\"\"\n",
        "    Applys transformation to training sample\n",
        "    \"\"\"\n",
        "    def __init__(self, cardinality, seeds=None, name='sparse'):\n",
        "        assert seeds is not None\n",
        "        self._cardinality = cardinality\n",
        "        self._name = name\n",
        "        self._seeds = torch.tensor(seeds)\n",
        "\n",
        "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
        "        sample[self._name] = (\n",
        "            (sample[self._name].unsqueeze(1) + self._seeds) % self._cardinality\n",
        "        ).long().reshape(-1)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ApiYoIQZVPXw"
      },
      "outputs": [],
      "source": [
        "seeds = [\n",
        "    [2342 + 13 * i, 7777 + 17 * i]\n",
        "    for i in range(26)\n",
        "]\n",
        "transform = MultihashTransform(10, seeds)\n",
        "input = {\n",
        "    'label': torch.tensor(1),\n",
        "    'dense': torch.randn(13),\n",
        "    'sparse': torch.arange(26)\n",
        "}\n",
        "output = transform(input)\n",
        "assert output['sparse'].shape == (2 * 26,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dch3lQsASC9N",
        "outputId": "6e8741f4-4660-4f06-c2a7-64dc0356532b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5,\n",
            "        0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1, 8, 9, 2, 7, 6, 5, 0, 3, 4, 1,\n",
            "        8, 9, 2, 7])\n"
          ]
        }
      ],
      "source": [
        "print(output['sparse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LvGGvGaoV99w"
      },
      "outputs": [],
      "source": [
        "class UnifiedEmbeddings(nn.Module):\n",
        "    def __init__(self, cardinality, embedding_dim):\n",
        "        super().__init__()\n",
        "        self._cardinality = cardinality\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=cardinality, embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, ids: torch.tensor):\n",
        "        # ids shape: [batch_size, num_features]\n",
        "        return self.embeddings(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItOaKUJYvn4w"
      },
      "source": [
        "## 3. Picewise Linear Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEQ_5waVSC9O"
      },
      "source": [
        "# Piecewise Linear Encoding (PLE) for Numerical Features\n",
        "\n",
        "We use *Piecewise Linear Encoding (PLE)* from Yandex Research:\n",
        "“On Embeddings for Numerical Features in Tabular Deep Learning”\n",
        "https://arxiv.org/abs/2203.05556\n",
        "\n",
        "GitHub implementation:\n",
        "https://github.com/yandex-research/rtdl-num-embeddings\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "Numerical features are embedded using functions of the form:\n",
        "\n",
        "    z_i = f_i(x_i^(num)) ∈ R^{d_i}\n",
        "\n",
        "Where:\n",
        "- f_i(x) — embedding function for the i-th numerical feature\n",
        "- z_i — resulting embedding vector\n",
        "- d_i — embedding dimension for that feature\n",
        "\n",
        "### Key properties:\n",
        "- Each numerical feature is embedded independently\n",
        "- In MLP architectures, embeddings are concatenated\n",
        "- In Transformer architectures, embeddings are used directly (as tokens)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Piecewise Linear Encoding (PLE)\n",
        "\n",
        "PLE divides the numerical feature range into T bins:\n",
        "\n",
        "    [b_0, b_1], [b_1, b_2], …, [b_{T−1}, b_T]\n",
        "\n",
        "The encoding is:\n",
        "\n",
        "    PLE(x) = [e_1, e_2, ..., e_T] ∈ R^T\n",
        "\n",
        "Each element e_t is defined as:\n",
        "\n",
        "                0,                           if x < b_{t−1} and t > 1\n",
        "    e_t(x) =    1,                           if x ≥ b_t     and t < T\n",
        "                (x − b_{t−1}) / (b_t − b_{t−1}), otherwise\n",
        "\n",
        "### Important properties:\n",
        "- When T = 1, PLE reduces to the raw scalar (identity transform)\n",
        "- Unlike categorical embeddings, PLE respects ordering\n",
        "- PLE behaves similarly to *learnable preprocessing*\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Using PLE in Transformer Models\n",
        "\n",
        "Transformers require fixed-dimensional vector tokens.\n",
        "\n",
        "For each bin B_t:\n",
        "- we learn an embedding vector v_t ∈ R^d\n",
        "\n",
        "The final embedding for a numerical feature is:\n",
        "\n",
        "    f_i(x) = v_0 + Σ_{t=1}^T e_t(x) · v_t\n",
        "           = Linear(PLE(x))\n",
        "\n",
        "This makes PLE directly compatible with attention-based models.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. How to Choose the Bin Boundaries\n",
        "\n",
        "The standard approach is quantile binning:\n",
        "\n",
        "    b_t = q_t( {x_j^(num)} over training set )\n",
        "\n",
        "Where q_t is the empirical quantile.\n",
        "\n",
        "This ensures:\n",
        "- bins contain similar numbers of samples\n",
        "- continuous features are encoded smoothly\n",
        "- rare extreme values do not collapse into a single region\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "PLE offers:\n",
        "- Smooth and expressive numerical embeddings\n",
        "- Better performance than raw numeric inputs or normalization\n",
        "- Easy integration into both MLPs and Transformers\n",
        "- Learnable representation that respects feature ordering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cJczvUQvfnI7"
      },
      "outputs": [],
      "source": [
        "class PiecewiseLinearEncodingTransform:\n",
        "    \"\"\"\n",
        "    Applys transformation to training sample\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def compute_bins(\n",
        "        X: torch.Tensor,\n",
        "        n_bins: int,\n",
        "    ) -> list[torch.Tensor]:\n",
        "        bins = [\n",
        "            q.unique() # 1D tensor of quantile boundaries\n",
        "            for q in torch.quantile(\n",
        "                X, torch.linspace(0.0, 1.0, n_bins + 1).to(X), dim=0\n",
        "            ).T\n",
        "        ]\n",
        "        return bins\n",
        "\n",
        "    def __init__(self, dense_train_df, n_bins=32, name='dense'):\n",
        "        self._name = name\n",
        "        self._bins = PiecewiseLinearEncodingTransform.compute_bins(dense_train_df.to_torch(), n_bins)\n",
        "        n_features = len(self._bins)\n",
        "        self._n_bins = [len(x) - 1 for x in self._bins]\n",
        "        max_n_bins = max(self._n_bins)\n",
        "\n",
        "        self.weight = torch.zeros(n_features, max_n_bins)\n",
        "        self.bias = torch.zeros(n_features, max_n_bins)\n",
        "\n",
        "        for i, bin_edges in enumerate(self._bins):\n",
        "            bin_width = bin_edges.diff()\n",
        "            w = 1.0 / bin_width\n",
        "            b = -bin_edges[:-1] / bin_width\n",
        "            self.weight[i, -1] = w[-1]\n",
        "            self.bias[i, -1] = b[-1]\n",
        "            self.weight[i, :self._n_bins[i] - 1] = w[:-1]\n",
        "            self.bias[i, :self._n_bins[i] - 1] = b[:-1]\n",
        "\n",
        "    @property\n",
        "    def n_bins(self):\n",
        "        return self._n_bins\n",
        "\n",
        "    def __call__(self, sample: dict[str, tp.Any]) -> dict[str, tp.Any]:\n",
        "        x = sample[self._name].to(torch.float32).unsqueeze(0)\n",
        "        x = torch.addcmul(self.bias, self.weight, x[..., None])\n",
        "        x = torch.cat(\n",
        "            [\n",
        "                x[..., :1].clamp_max(1.0), # leftmost bin\n",
        "                x[..., 1:-1].clamp(0.0, 1.0), # middle bins\n",
        "                x[..., -1:].clamp_min(0.0) # rightmost bin\n",
        "            ],\n",
        "            dim=-1,\n",
        "        )\n",
        "        x = x.flatten(-2).squeeze(0)\n",
        "        sample[self._name] = x\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG3yBqX1SC9O"
      },
      "source": [
        "Example:\n",
        "\n",
        "**weight** =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{b_1 - b_0} & \\frac{1}{b_2 - b_1} & \\frac{1}{b_3 - b_2} & \\frac{1}{b_4 - b_3} \\\\\n",
        "\\frac{1}{c_1 - c_0} & \\frac{1}{c_2 - c_1} & \\frac{1}{c_3 - c_2} & \\frac{1}{c_4 - c_3}\n",
        "\\end{bmatrix}\n",
        "\n",
        "**bias** =\n",
        "\\begin{bmatrix}\n",
        "-\\frac{b_0}{b_1 - b_0} & -\\frac{b_1}{b_2 - b_1} & -\\frac{b_2}{b_3 - b_2} & -\\frac{b_3}{b_4 - b_3} \\\\\n",
        "-\\frac{c_0}{c_1 - c_0} & -\\frac{c_1}{c_2 - c_1} & -\\frac{c_2}{c_3 - c_2} & -\\frac{c_3}{c_4 - c_3}\n",
        "\\end{bmatrix}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ruDgPcWASC9P"
      },
      "outputs": [],
      "source": [
        "transform = PiecewiseLinearEncodingTransform(train_df[CriteoDatasetUtils.INT_COLS], name='dense')\n",
        "input = {\n",
        "    'label': torch.tensor(1),\n",
        "    'dense': torch.randn(13),\n",
        "    'sparse': torch.arange(26)\n",
        "}\n",
        "output = transform(input)\n",
        "assert output['dense'].shape == (403,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c6-fc_dxhx6R"
      },
      "outputs": [],
      "source": [
        "class PiecewiseLinearEncoding(nn.Identity):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5y1zKpv0L3"
      },
      "source": [
        "## 4. DCN v2 - deep cross network\n",
        "\n",
        "For the aggregation of categorical and numerical features into one scalar, we're going to be using DCNv2 from Google DeepMind: [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nr8_LFSC9P"
      },
      "source": [
        "### Approach\n",
        "\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/ZqfF5yf/dcn-v2.png)\n",
        "![](https://i.ibb.co/SDYWNSMy/dcn-v2-equation.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Stacked variant:\n",
        "\n",
        "- First, a sequence of cross layers:\n",
        "  $$\n",
        "  x_{i+1} = x_0 \\odot (W x_i + b) + x_i.\n",
        "  $$\n",
        "\n",
        "- Then, a sequence of deep (fully connected) layers:\n",
        "  $$\n",
        "  h_{l+1} = f(W_l h_l + b_l).\n",
        "  $$\n",
        "\n",
        "\n",
        "### Why it works and what it’s for\n",
        "\n",
        "- We know that explicit feature crosses (feature–feature interactions) are important in many recommendation and ranking tasks.\n",
        "- A plain DNN mostly learns implicit interactions and can approximate dot-products and higher-order crosses only with deep, wide networks, which are expensive at inference time.\n",
        "- CrossNet adds explicit, low-rank feature interactions via the cross layers.  \n",
        "  This reduces the need for very deep DNN towers, making the model:\n",
        "  - easier to train,\n",
        "  - cheaper to serve in production,\n",
        "  - and better aligned with tasks where cross features matter a lot (e.g., user–item, user–context interactions).\n",
        "\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/K34HqJH/dcn-theory.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TvRu94iB-DSt"
      },
      "outputs": [],
      "source": [
        "class CrossLayer(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x0, xl):\n",
        "        return x0 * self.linear(xl) + xl\n",
        "\n",
        "\n",
        "class CrossNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([CrossLayer(input_dim) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        xl = x\n",
        "        for layer in self.layers:\n",
        "            xl = layer(x, xl)\n",
        "        return xl\n",
        "\n",
        "\n",
        "class DeepNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_units):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for units in hidden_units:\n",
        "            layers.append(nn.Linear(input_dim, units))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class DCNV2(nn.Module):\n",
        "    def __init__(self, embedding_size, cross_layers, deep_units, input_size, cardinality=65536):\n",
        "        super().__init__()\n",
        "        self.sparse_encode_layer = UnifiedEmbeddings(cardinality, embedding_size)\n",
        "        self.dense_encode_layer = PiecewiseLinearEncoding()\n",
        "        self.cross_network = CrossNetwork(input_size, cross_layers)\n",
        "        self.deep_network = DeepNetwork(input_size, deep_units)\n",
        "        self.output_layer = nn.Linear(deep_units[-1], 1)\n",
        "\n",
        "    def forward(self, dense_input, sparse_input):\n",
        "        sparse_embeddings = self.sparse_encode_layer(sparse_input).view(sparse_input.size(0), -1)\n",
        "        dense_embeddings = self.dense_encode_layer(dense_input)\n",
        "        combined_input = torch.cat([dense_embeddings, sparse_embeddings], dim=-1)\n",
        "        cross_output = self.cross_network(combined_input)\n",
        "        deep_output = self.deep_network(cross_output)\n",
        "        return self.output_layer(deep_output).squeeze(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGt5jk-Zv3fW"
      },
      "source": [
        "## 5. Training Neural Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7SqHAgI6DbA_"
      },
      "outputs": [],
      "source": [
        "class CriteoDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            df: pl.DataFrame,\n",
        "            transforms: list[tp.Callable[[tp.Any], tp.Any]] = None,\n",
        "    ):\n",
        "        self._labels = df[CriteoDatasetUtils.LABEL_COl].to_torch().to(torch.float32)\n",
        "        self._dense = df[CriteoDatasetUtils.INT_COLS].to_torch()\n",
        "        self._sparse = df[CriteoDatasetUtils.CAT_COLS].to_torch()\n",
        "        self._transforms = (\n",
        "            transforms if transforms is not None else []\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._labels.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'label': self._labels[idx],\n",
        "            'dense_features': self._dense[idx],\n",
        "            'sparse_features': self._sparse[idx]\n",
        "        }\n",
        "        for transform in self._transforms:\n",
        "            sample = transform(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VFVYcvxlSC9P"
      },
      "outputs": [],
      "source": [
        "batch_size = 4096\n",
        "cardinality = 8 * 65536\n",
        "seeds = [[2342 + 13 * i, 7777 + 17 * i, 131 + 833 * i] for i in range(len(CriteoDatasetUtils.CAT_COLS))]\n",
        "num_hashes = 3\n",
        "embedding_size = 64\n",
        "n_bins = 39"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LagiiLf50Yer"
      },
      "outputs": [],
      "source": [
        "dense_transform = PiecewiseLinearEncodingTransform(train_df[CriteoDatasetUtils.INT_COLS], n_bins, name='dense_features')\n",
        "sparse_transform = MultihashTransform(cardinality, seeds, name='sparse_features')\n",
        "transforms = [dense_transform, sparse_transform]\n",
        "\n",
        "train_dataset = CriteoDataset(train_df, transforms)\n",
        "val_dataset = CriteoDataset(test_df, transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rMCqyw4qzw7H"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(int_features, cat_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        all_scores, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader):\n",
        "                int_features, cat_features, labels = batch['dense_features'].to(device), batch['sparse_features'].to(device), batch['label'].to(device)\n",
        "\n",
        "                outputs = model(int_features, cat_features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                all_scores.append(outputs.clone().cpu())\n",
        "                all_labels.append(labels.clone().cpu())\n",
        "            all_scores = torch.cat(all_scores, dim=-1)\n",
        "            all_labels = torch.cat(all_labels, dim=-1)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, '\n",
        "              f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100*correct/total:.2f}%, '\n",
        "              f'Val ROC AUC: {roc_auc_score(all_labels, all_scores)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i6HlVLei0Fl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529c6050-1d00-4e2c-d530-188557b3ab7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5486\n"
          ]
        }
      ],
      "source": [
        "input_size = max(dense_transform.n_bins) * len(CriteoDatasetUtils.INT_COLS) + num_hashes * embedding_size * len(CriteoDatasetUtils.CAT_COLS)\n",
        "model = DCNV2(\n",
        "    embedding_size=embedding_size,\n",
        "    cross_layers=3,\n",
        "    deep_units=[1024, 1024, 1024],\n",
        "    input_size=input_size,\n",
        "    cardinality=cardinality,\n",
        ")\n",
        "print(input_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, epochs=1, lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm9AXhqo6_20",
        "outputId": "858f3931-97c0-486a-ad8e-51ae106a6970"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 960/960 [11:31<00:00,  1.39it/s]\n",
            "100%|██████████| 160/160 [01:18<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 0.4660, Val Loss: 0.4611, Accuracy: 77.69%, Val ROC AUC: 0.790797975158338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz2Ec80fwU3l"
      },
      "source": [
        "## 6. Comparing with CatBoost (same features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPkGZ_v0B5--",
        "outputId": "fa136763-b605-408c-9b05-963b08477799"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool"
      ],
      "metadata": {
        "id": "4GXEjAq5DrHH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YcTIK0HleqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081d6c40-24ec-4dfa-a04c-883f70ef329e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.035238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.7171938\tbest: 0.7171938 (0)\ttotal: 2.54s\tremaining: 42m 20s\n",
            "1:\ttotal: 4.94s\tremaining: 41m 7s\n",
            "2:\ttotal: 7.42s\tremaining: 41m 7s\n",
            "3:\ttotal: 9.82s\tremaining: 40m 45s\n",
            "4:\ttotal: 12.3s\tremaining: 40m 38s\n",
            "5:\ttest: 0.7310488\tbest: 0.7310488 (5)\ttotal: 14.7s\tremaining: 40m 40s\n",
            "6:\ttotal: 16.3s\tremaining: 38m 26s\n",
            "7:\ttotal: 18.8s\tremaining: 38m 53s\n",
            "8:\ttotal: 21.4s\tremaining: 39m 14s\n",
            "9:\ttotal: 23.6s\tremaining: 38m 54s\n",
            "10:\ttest: 0.7424861\tbest: 0.7424861 (10)\ttotal: 26s\tremaining: 39m\n",
            "11:\ttotal: 28.5s\tremaining: 39m 6s\n",
            "12:\ttotal: 31s\tremaining: 39m 11s\n",
            "13:\ttotal: 33.4s\tremaining: 39m 14s\n",
            "14:\ttotal: 35.9s\tremaining: 39m 19s\n",
            "15:\ttest: 0.7461343\tbest: 0.7461343 (15)\ttotal: 37.6s\tremaining: 38m 30s\n",
            "16:\ttotal: 39.7s\tremaining: 38m 13s\n",
            "17:\ttotal: 42.1s\tremaining: 38m 16s\n",
            "18:\ttotal: 44.3s\tremaining: 38m 6s\n",
            "19:\ttotal: 46.4s\tremaining: 37m 54s\n",
            "20:\ttest: 0.7499114\tbest: 0.7499114 (20)\ttotal: 49.1s\tremaining: 38m 9s\n",
            "21:\ttotal: 51.3s\tremaining: 37m 58s\n",
            "22:\ttotal: 53.4s\tremaining: 37m 47s\n",
            "23:\ttotal: 55.5s\tremaining: 37m 37s\n",
            "24:\ttotal: 57.9s\tremaining: 37m 36s\n",
            "25:\ttest: 0.7527250\tbest: 0.7527250 (25)\ttotal: 1m\tremaining: 37m 42s\n",
            "26:\ttotal: 1m 2s\tremaining: 37m 20s\n",
            "27:\ttotal: 1m 4s\tremaining: 37m 24s\n",
            "28:\ttotal: 1m 7s\tremaining: 37m 33s\n",
            "29:\ttotal: 1m 9s\tremaining: 37m 26s\n",
            "30:\ttest: 0.7553180\tbest: 0.7553180 (30)\ttotal: 1m 11s\tremaining: 37m 13s\n",
            "31:\ttotal: 1m 13s\tremaining: 37m 17s\n",
            "32:\ttotal: 1m 16s\tremaining: 37m 21s\n",
            "33:\ttotal: 1m 18s\tremaining: 37m 13s\n",
            "34:\ttotal: 1m 20s\tremaining: 37m 5s\n",
            "35:\ttest: 0.7574898\tbest: 0.7574898 (35)\ttotal: 1m 22s\tremaining: 37m 2s\n",
            "36:\ttotal: 1m 25s\tremaining: 37m 3s\n",
            "37:\ttotal: 1m 27s\tremaining: 36m 55s\n",
            "38:\ttotal: 1m 30s\tremaining: 37m\n",
            "39:\ttotal: 1m 32s\tremaining: 37m 2s\n",
            "40:\ttest: 0.7589086\tbest: 0.7589086 (40)\ttotal: 1m 34s\tremaining: 37m\n",
            "41:\ttotal: 1m 36s\tremaining: 36m 51s\n",
            "42:\ttotal: 1m 39s\tremaining: 36m 54s\n",
            "43:\ttotal: 1m 42s\tremaining: 36m 56s\n",
            "44:\ttotal: 1m 44s\tremaining: 36m 56s\n",
            "45:\ttest: 0.7602838\tbest: 0.7602838 (45)\ttotal: 1m 46s\tremaining: 36m 55s\n",
            "46:\ttotal: 1m 49s\tremaining: 36m 50s\n",
            "47:\ttotal: 1m 51s\tremaining: 36m 41s\n",
            "48:\ttotal: 1m 52s\tremaining: 36m 32s\n",
            "49:\ttotal: 1m 55s\tremaining: 36m 26s\n",
            "50:\ttest: 0.7616951\tbest: 0.7616951 (50)\ttotal: 1m 57s\tremaining: 36m 17s\n",
            "51:\ttotal: 1m 59s\tremaining: 36m 20s\n",
            "52:\ttotal: 2m 1s\tremaining: 36m 16s\n",
            "53:\ttotal: 2m 4s\tremaining: 36m 15s\n",
            "54:\ttotal: 2m 6s\tremaining: 36m 11s\n",
            "55:\ttest: 0.7628089\tbest: 0.7628089 (55)\ttotal: 2m 8s\tremaining: 36m 5s\n",
            "56:\ttotal: 2m 10s\tremaining: 35m 52s\n",
            "57:\ttotal: 2m 12s\tremaining: 35m 47s\n",
            "58:\ttotal: 2m 14s\tremaining: 35m 40s\n",
            "59:\ttotal: 2m 16s\tremaining: 35m 34s\n",
            "60:\ttest: 0.7638158\tbest: 0.7638158 (60)\ttotal: 2m 18s\tremaining: 35m 28s\n",
            "61:\ttotal: 2m 20s\tremaining: 35m 22s\n",
            "62:\ttotal: 2m 22s\tremaining: 35m 18s\n",
            "63:\ttotal: 2m 24s\tremaining: 35m 8s\n",
            "64:\ttotal: 2m 26s\tremaining: 35m 4s\n",
            "65:\ttest: 0.7647552\tbest: 0.7647552 (65)\ttotal: 2m 28s\tremaining: 35m 4s\n",
            "66:\ttotal: 2m 31s\tremaining: 35m 5s\n",
            "67:\ttotal: 2m 33s\tremaining: 34m 58s\n",
            "68:\ttotal: 2m 35s\tremaining: 34m 55s\n",
            "69:\ttotal: 2m 37s\tremaining: 34m 52s\n",
            "70:\ttest: 0.7657200\tbest: 0.7657200 (70)\ttotal: 2m 39s\tremaining: 34m 45s\n",
            "71:\ttotal: 2m 41s\tremaining: 34m 35s\n",
            "72:\ttotal: 2m 43s\tremaining: 34m 33s\n",
            "73:\ttotal: 2m 45s\tremaining: 34m 28s\n",
            "74:\ttotal: 2m 47s\tremaining: 34m 23s\n",
            "75:\ttest: 0.7664645\tbest: 0.7664645 (75)\ttotal: 2m 49s\tremaining: 34m 19s\n",
            "76:\ttotal: 2m 51s\tremaining: 34m 19s\n",
            "77:\ttotal: 2m 53s\tremaining: 34m 15s\n",
            "78:\ttotal: 2m 55s\tremaining: 34m 9s\n",
            "79:\ttotal: 2m 57s\tremaining: 33m 59s\n",
            "80:\ttest: 0.7671964\tbest: 0.7671964 (80)\ttotal: 2m 59s\tremaining: 33m 56s\n",
            "81:\ttotal: 3m 1s\tremaining: 33m 57s\n",
            "82:\ttotal: 3m 4s\tremaining: 33m 53s\n",
            "83:\ttotal: 3m 6s\tremaining: 33m 48s\n",
            "84:\ttotal: 3m 7s\tremaining: 33m 39s\n",
            "85:\ttest: 0.7678958\tbest: 0.7678958 (85)\ttotal: 3m 9s\tremaining: 33m 36s\n",
            "86:\ttotal: 3m 11s\tremaining: 33m 33s\n",
            "87:\ttotal: 3m 13s\tremaining: 33m 30s\n",
            "88:\ttotal: 3m 16s\tremaining: 33m 29s\n",
            "89:\ttotal: 3m 18s\tremaining: 33m 27s\n",
            "90:\ttest: 0.7685434\tbest: 0.7685434 (90)\ttotal: 3m 20s\tremaining: 33m 19s\n",
            "91:\ttotal: 3m 21s\tremaining: 33m 11s\n",
            "92:\ttotal: 3m 23s\tremaining: 33m 8s\n",
            "93:\ttotal: 3m 25s\tremaining: 33m 4s\n",
            "94:\ttotal: 3m 28s\tremaining: 33m 5s\n",
            "95:\ttest: 0.7691271\tbest: 0.7691271 (95)\ttotal: 3m 29s\tremaining: 32m 57s\n",
            "96:\ttotal: 3m 31s\tremaining: 32m 51s\n",
            "97:\ttotal: 3m 33s\tremaining: 32m 47s\n",
            "98:\ttotal: 3m 36s\tremaining: 32m 49s\n",
            "99:\ttotal: 3m 38s\tremaining: 32m 48s\n",
            "100:\ttest: 0.7698497\tbest: 0.7698497 (100)\ttotal: 3m 40s\tremaining: 32m 40s\n",
            "101:\ttotal: 3m 42s\tremaining: 32m 37s\n",
            "102:\ttotal: 3m 44s\tremaining: 32m 37s\n",
            "103:\ttotal: 3m 46s\tremaining: 32m 33s\n",
            "104:\ttotal: 3m 48s\tremaining: 32m 26s\n",
            "105:\ttest: 0.7703855\tbest: 0.7703855 (105)\ttotal: 3m 50s\tremaining: 32m 25s\n",
            "106:\ttotal: 3m 52s\tremaining: 32m 18s\n",
            "107:\ttotal: 3m 53s\tremaining: 32m 11s\n",
            "108:\ttotal: 3m 55s\tremaining: 32m 9s\n",
            "109:\ttotal: 3m 57s\tremaining: 32m 5s\n",
            "110:\ttest: 0.7710642\tbest: 0.7710642 (110)\ttotal: 3m 59s\tremaining: 32m 1s\n",
            "111:\ttotal: 4m 1s\tremaining: 31m 54s\n",
            "112:\ttotal: 4m 3s\tremaining: 31m 53s\n",
            "113:\ttotal: 4m 5s\tremaining: 31m 49s\n",
            "114:\ttotal: 4m 7s\tremaining: 31m 43s\n",
            "115:\ttest: 0.7716813\tbest: 0.7716813 (115)\ttotal: 4m 9s\tremaining: 31m 44s\n",
            "116:\ttotal: 4m 12s\tremaining: 31m 42s\n",
            "117:\ttotal: 4m 13s\tremaining: 31m 36s\n",
            "118:\ttotal: 4m 16s\tremaining: 31m 36s\n",
            "119:\ttotal: 4m 18s\tremaining: 31m 36s\n",
            "120:\ttest: 0.7721553\tbest: 0.7721553 (120)\ttotal: 4m 20s\tremaining: 31m 34s\n",
            "121:\ttotal: 4m 22s\tremaining: 31m 30s\n",
            "122:\ttotal: 4m 24s\tremaining: 31m 27s\n",
            "123:\ttotal: 4m 26s\tremaining: 31m 24s\n",
            "124:\ttotal: 4m 28s\tremaining: 31m 20s\n",
            "125:\ttest: 0.7725731\tbest: 0.7725731 (125)\ttotal: 4m 30s\tremaining: 31m 14s\n",
            "126:\ttotal: 4m 32s\tremaining: 31m 11s\n",
            "127:\ttotal: 4m 34s\tremaining: 31m 6s\n",
            "128:\ttotal: 4m 35s\tremaining: 31m 1s\n",
            "129:\ttotal: 4m 37s\tremaining: 30m 58s\n",
            "130:\ttest: 0.7730993\tbest: 0.7730993 (130)\ttotal: 4m 39s\tremaining: 30m 52s\n",
            "131:\ttotal: 4m 41s\tremaining: 30m 49s\n",
            "132:\ttotal: 4m 43s\tremaining: 30m 44s\n",
            "133:\ttotal: 4m 45s\tremaining: 30m 43s\n",
            "134:\ttotal: 4m 47s\tremaining: 30m 43s\n",
            "135:\ttest: 0.7734705\tbest: 0.7734705 (135)\ttotal: 4m 49s\tremaining: 30m 40s\n",
            "136:\ttotal: 4m 51s\tremaining: 30m 38s\n",
            "137:\ttotal: 4m 54s\tremaining: 30m 39s\n",
            "138:\ttotal: 4m 56s\tremaining: 30m 35s\n",
            "139:\ttotal: 4m 58s\tremaining: 30m 31s\n",
            "140:\ttest: 0.7738192\tbest: 0.7738192 (140)\ttotal: 4m 59s\tremaining: 30m 26s\n",
            "141:\ttotal: 5m 2s\tremaining: 30m 26s\n",
            "142:\ttotal: 5m 4s\tremaining: 30m 22s\n",
            "143:\ttotal: 5m 5s\tremaining: 30m 18s\n",
            "144:\ttotal: 5m 7s\tremaining: 30m 13s\n",
            "145:\ttest: 0.7741842\tbest: 0.7741842 (145)\ttotal: 5m 9s\tremaining: 30m 9s\n",
            "146:\ttotal: 5m 11s\tremaining: 30m 4s\n",
            "147:\ttotal: 5m 12s\tremaining: 30m\n",
            "148:\ttotal: 5m 14s\tremaining: 29m 55s\n",
            "149:\ttotal: 5m 16s\tremaining: 29m 52s\n",
            "150:\ttest: 0.7746827\tbest: 0.7746827 (150)\ttotal: 5m 18s\tremaining: 29m 51s\n",
            "151:\ttotal: 5m 20s\tremaining: 29m 48s\n",
            "152:\ttotal: 5m 22s\tremaining: 29m 47s\n",
            "153:\ttotal: 5m 24s\tremaining: 29m 44s\n",
            "154:\ttotal: 5m 26s\tremaining: 29m 42s\n",
            "155:\ttest: 0.7750743\tbest: 0.7750743 (155)\ttotal: 5m 28s\tremaining: 29m 37s\n",
            "156:\ttotal: 5m 30s\tremaining: 29m 33s\n",
            "157:\ttotal: 5m 32s\tremaining: 29m 32s\n",
            "158:\ttotal: 5m 34s\tremaining: 29m 29s\n",
            "159:\ttotal: 5m 37s\tremaining: 29m 29s\n",
            "160:\ttest: 0.7753573\tbest: 0.7753573 (160)\ttotal: 5m 38s\tremaining: 29m 26s\n",
            "161:\ttotal: 5m 40s\tremaining: 29m 22s\n",
            "162:\ttotal: 5m 42s\tremaining: 29m 17s\n",
            "163:\ttotal: 5m 44s\tremaining: 29m 17s\n",
            "164:\ttotal: 5m 47s\tremaining: 29m 16s\n",
            "165:\ttest: 0.7756604\tbest: 0.7756604 (165)\ttotal: 5m 49s\tremaining: 29m 14s\n",
            "166:\ttotal: 5m 50s\tremaining: 29m 10s\n",
            "167:\ttotal: 5m 52s\tremaining: 29m 6s\n",
            "168:\ttotal: 5m 54s\tremaining: 29m 4s\n",
            "169:\ttotal: 5m 56s\tremaining: 28m 59s\n",
            "170:\ttest: 0.7760997\tbest: 0.7760997 (170)\ttotal: 5m 57s\tremaining: 28m 54s\n",
            "171:\ttotal: 6m\tremaining: 28m 54s\n",
            "172:\ttotal: 6m 1s\tremaining: 28m 49s\n",
            "173:\ttotal: 6m 3s\tremaining: 28m 45s\n",
            "174:\ttotal: 6m 4s\tremaining: 28m 40s\n",
            "175:\ttest: 0.7763221\tbest: 0.7763221 (175)\ttotal: 6m 7s\tremaining: 28m 39s\n",
            "176:\ttotal: 6m 9s\tremaining: 28m 39s\n",
            "177:\ttotal: 6m 11s\tremaining: 28m 35s\n",
            "178:\ttotal: 6m 13s\tremaining: 28m 33s\n",
            "179:\ttotal: 6m 15s\tremaining: 28m 31s\n",
            "180:\ttest: 0.7766131\tbest: 0.7766131 (180)\ttotal: 6m 17s\tremaining: 28m 26s\n",
            "181:\ttotal: 6m 19s\tremaining: 28m 25s\n",
            "182:\ttotal: 6m 22s\tremaining: 28m 25s\n",
            "183:\ttotal: 6m 24s\tremaining: 28m 25s\n",
            "184:\ttotal: 6m 26s\tremaining: 28m 21s\n",
            "185:\ttest: 0.7768363\tbest: 0.7768363 (185)\ttotal: 6m 27s\tremaining: 28m 17s\n",
            "186:\ttotal: 6m 30s\tremaining: 28m 16s\n",
            "187:\ttotal: 6m 32s\tremaining: 28m 14s\n",
            "188:\ttotal: 6m 33s\tremaining: 28m 10s\n",
            "189:\ttotal: 6m 35s\tremaining: 28m 6s\n",
            "190:\ttest: 0.7770584\tbest: 0.7770584 (190)\ttotal: 6m 37s\tremaining: 28m 5s\n",
            "191:\ttotal: 6m 40s\tremaining: 28m 4s\n",
            "192:\ttotal: 6m 42s\tremaining: 28m 1s\n",
            "193:\ttotal: 6m 43s\tremaining: 27m 57s\n",
            "194:\ttotal: 6m 45s\tremaining: 27m 54s\n",
            "195:\ttest: 0.7773787\tbest: 0.7773787 (195)\ttotal: 6m 47s\tremaining: 27m 52s\n",
            "196:\ttotal: 6m 49s\tremaining: 27m 50s\n",
            "197:\ttotal: 6m 51s\tremaining: 27m 46s\n",
            "198:\ttotal: 6m 53s\tremaining: 27m 45s\n",
            "199:\ttotal: 6m 55s\tremaining: 27m 41s\n",
            "200:\ttest: 0.7775351\tbest: 0.7775351 (200)\ttotal: 6m 56s\tremaining: 27m 37s\n",
            "201:\ttotal: 6m 59s\tremaining: 27m 35s\n",
            "202:\ttotal: 7m\tremaining: 27m 31s\n",
            "203:\ttotal: 7m 2s\tremaining: 27m 28s\n",
            "204:\ttotal: 7m 4s\tremaining: 27m 25s\n",
            "205:\ttest: 0.7777853\tbest: 0.7777853 (205)\ttotal: 7m 5s\tremaining: 27m 21s\n",
            "206:\ttotal: 7m 8s\tremaining: 27m 20s\n",
            "207:\ttotal: 7m 10s\tremaining: 27m 19s\n",
            "208:\ttotal: 7m 12s\tremaining: 27m 18s\n",
            "209:\ttotal: 7m 15s\tremaining: 27m 16s\n",
            "210:\ttest: 0.7779726\tbest: 0.7779726 (210)\ttotal: 7m 16s\tremaining: 27m 13s\n",
            "211:\ttotal: 7m 19s\tremaining: 27m 11s\n",
            "212:\ttotal: 7m 20s\tremaining: 27m 8s\n",
            "213:\ttotal: 7m 22s\tremaining: 27m 4s\n",
            "214:\ttotal: 7m 24s\tremaining: 27m 1s\n",
            "215:\ttest: 0.7782226\tbest: 0.7782226 (215)\ttotal: 7m 25s\tremaining: 26m 57s\n",
            "216:\ttotal: 7m 28s\tremaining: 26m 56s\n",
            "217:\ttotal: 7m 30s\tremaining: 26m 55s\n",
            "218:\ttotal: 7m 32s\tremaining: 26m 54s\n",
            "219:\ttotal: 7m 34s\tremaining: 26m 52s\n",
            "220:\ttest: 0.7784144\tbest: 0.7784144 (220)\ttotal: 7m 36s\tremaining: 26m 48s\n",
            "221:\ttotal: 7m 38s\tremaining: 26m 47s\n",
            "222:\ttotal: 7m 40s\tremaining: 26m 43s\n",
            "223:\ttotal: 7m 41s\tremaining: 26m 39s\n",
            "224:\ttotal: 7m 43s\tremaining: 26m 36s\n",
            "225:\ttest: 0.7786430\tbest: 0.7786430 (225)\ttotal: 7m 45s\tremaining: 26m 34s\n",
            "226:\ttotal: 7m 47s\tremaining: 26m 31s\n",
            "227:\ttotal: 7m 49s\tremaining: 26m 30s\n",
            "228:\ttotal: 7m 51s\tremaining: 26m 26s\n",
            "229:\ttotal: 7m 53s\tremaining: 26m 25s\n",
            "230:\ttest: 0.7788160\tbest: 0.7788160 (230)\ttotal: 7m 55s\tremaining: 26m 22s\n",
            "231:\ttotal: 7m 57s\tremaining: 26m 19s\n",
            "232:\ttotal: 7m 59s\tremaining: 26m 19s\n",
            "233:\ttotal: 8m 1s\tremaining: 26m 16s\n",
            "234:\ttotal: 8m 3s\tremaining: 26m 13s\n",
            "235:\ttest: 0.7790197\tbest: 0.7790197 (235)\ttotal: 8m 5s\tremaining: 26m 10s\n",
            "236:\ttotal: 8m 7s\tremaining: 26m 9s\n",
            "237:\ttotal: 8m 9s\tremaining: 26m 7s\n",
            "238:\ttotal: 8m 11s\tremaining: 26m 6s\n",
            "239:\ttotal: 8m 13s\tremaining: 26m 2s\n",
            "240:\ttest: 0.7792415\tbest: 0.7792415 (240)\ttotal: 8m 15s\tremaining: 25m 59s\n",
            "241:\ttotal: 8m 16s\tremaining: 25m 56s\n",
            "242:\ttotal: 8m 18s\tremaining: 25m 52s\n",
            "243:\ttotal: 8m 20s\tremaining: 25m 50s\n",
            "244:\ttotal: 8m 23s\tremaining: 25m 50s\n",
            "245:\ttest: 0.7794376\tbest: 0.7794376 (245)\ttotal: 8m 24s\tremaining: 25m 47s\n",
            "246:\ttotal: 8m 27s\tremaining: 25m 46s\n",
            "247:\ttotal: 8m 28s\tremaining: 25m 43s\n",
            "248:\ttotal: 8m 30s\tremaining: 25m 39s\n",
            "249:\ttotal: 8m 32s\tremaining: 25m 38s\n",
            "250:\ttest: 0.7795709\tbest: 0.7795709 (250)\ttotal: 8m 34s\tremaining: 25m 36s\n",
            "251:\ttotal: 8m 37s\tremaining: 25m 35s\n",
            "252:\ttotal: 8m 38s\tremaining: 25m 32s\n",
            "253:\ttotal: 8m 40s\tremaining: 25m 28s\n",
            "254:\ttotal: 8m 42s\tremaining: 25m 25s\n",
            "255:\ttest: 0.7798122\tbest: 0.7798122 (255)\ttotal: 8m 44s\tremaining: 25m 24s\n",
            "256:\ttotal: 8m 46s\tremaining: 25m 21s\n",
            "257:\ttotal: 8m 48s\tremaining: 25m 18s\n",
            "258:\ttotal: 8m 50s\tremaining: 25m 16s\n",
            "259:\ttotal: 8m 51s\tremaining: 25m 13s\n",
            "260:\ttest: 0.7800159\tbest: 0.7800159 (260)\ttotal: 8m 53s\tremaining: 25m 10s\n",
            "261:\ttotal: 8m 55s\tremaining: 25m 7s\n",
            "262:\ttotal: 8m 57s\tremaining: 25m 5s\n",
            "263:\ttotal: 8m 59s\tremaining: 25m 3s\n",
            "264:\ttotal: 9m\tremaining: 24m 59s\n",
            "265:\ttest: 0.7801977\tbest: 0.7801977 (265)\ttotal: 9m 2s\tremaining: 24m 57s\n",
            "266:\ttotal: 9m 4s\tremaining: 24m 54s\n",
            "267:\ttotal: 9m 5s\tremaining: 24m 51s\n",
            "268:\ttotal: 9m 8s\tremaining: 24m 50s\n",
            "269:\ttotal: 9m 10s\tremaining: 24m 47s\n",
            "270:\ttest: 0.7803193\tbest: 0.7803193 (270)\ttotal: 9m 11s\tremaining: 24m 44s\n",
            "271:\ttotal: 9m 13s\tremaining: 24m 41s\n",
            "272:\ttotal: 9m 15s\tremaining: 24m 38s\n",
            "273:\ttotal: 9m 17s\tremaining: 24m 37s\n",
            "274:\ttotal: 9m 19s\tremaining: 24m 34s\n",
            "275:\ttest: 0.7804865\tbest: 0.7804865 (275)\ttotal: 9m 21s\tremaining: 24m 32s\n",
            "276:\ttotal: 9m 23s\tremaining: 24m 30s\n",
            "277:\ttotal: 9m 25s\tremaining: 24m 27s\n",
            "278:\ttotal: 9m 27s\tremaining: 24m 26s\n",
            "279:\ttotal: 9m 29s\tremaining: 24m 23s\n",
            "280:\ttest: 0.7806514\tbest: 0.7806514 (280)\ttotal: 9m 30s\tremaining: 24m 20s\n",
            "281:\ttotal: 9m 32s\tremaining: 24m 18s\n",
            "282:\ttotal: 9m 34s\tremaining: 24m 15s\n",
            "283:\ttotal: 9m 36s\tremaining: 24m 13s\n",
            "284:\ttotal: 9m 38s\tremaining: 24m 11s\n",
            "285:\ttest: 0.7807919\tbest: 0.7807919 (285)\ttotal: 9m 40s\tremaining: 24m 9s\n",
            "286:\ttotal: 9m 42s\tremaining: 24m 7s\n",
            "287:\ttotal: 9m 44s\tremaining: 24m 6s\n",
            "288:\ttotal: 9m 47s\tremaining: 24m 4s\n",
            "289:\ttotal: 9m 49s\tremaining: 24m 2s\n",
            "290:\ttest: 0.7809100\tbest: 0.7809100 (290)\ttotal: 9m 51s\tremaining: 24m\n",
            "291:\ttotal: 9m 53s\tremaining: 23m 59s\n",
            "292:\ttotal: 9m 55s\tremaining: 23m 56s\n",
            "293:\ttotal: 9m 57s\tremaining: 23m 54s\n",
            "294:\ttotal: 9m 58s\tremaining: 23m 51s\n",
            "295:\ttest: 0.7810734\tbest: 0.7810734 (295)\ttotal: 10m\tremaining: 23m 48s\n",
            "296:\ttotal: 10m 2s\tremaining: 23m 45s\n",
            "297:\ttotal: 10m 4s\tremaining: 23m 44s\n",
            "298:\ttotal: 10m 6s\tremaining: 23m 43s\n",
            "299:\ttotal: 10m 8s\tremaining: 23m 39s\n",
            "300:\ttest: 0.7812008\tbest: 0.7812008 (300)\ttotal: 10m 9s\tremaining: 23m 36s\n",
            "301:\ttotal: 10m 11s\tremaining: 23m 33s\n",
            "302:\ttotal: 10m 13s\tremaining: 23m 31s\n",
            "303:\ttotal: 10m 15s\tremaining: 23m 29s\n",
            "304:\ttotal: 10m 17s\tremaining: 23m 27s\n",
            "305:\ttest: 0.7813253\tbest: 0.7813253 (305)\ttotal: 10m 19s\tremaining: 23m 24s\n",
            "306:\ttotal: 10m 21s\tremaining: 23m 23s\n",
            "307:\ttotal: 10m 23s\tremaining: 23m 21s\n",
            "308:\ttotal: 10m 25s\tremaining: 23m 19s\n",
            "309:\ttotal: 10m 28s\tremaining: 23m 18s\n",
            "310:\ttest: 0.7814787\tbest: 0.7814787 (310)\ttotal: 10m 29s\tremaining: 23m 15s\n",
            "311:\ttotal: 10m 31s\tremaining: 23m 12s\n",
            "312:\ttotal: 10m 33s\tremaining: 23m 10s\n",
            "313:\ttotal: 10m 35s\tremaining: 23m 8s\n",
            "314:\ttotal: 10m 38s\tremaining: 23m 7s\n",
            "315:\ttest: 0.7815818\tbest: 0.7815818 (315)\ttotal: 10m 39s\tremaining: 23m 4s\n",
            "316:\ttotal: 10m 41s\tremaining: 23m 2s\n",
            "317:\ttotal: 10m 44s\tremaining: 23m 1s\n",
            "318:\ttotal: 10m 45s\tremaining: 22m 58s\n",
            "319:\ttotal: 10m 47s\tremaining: 22m 56s\n",
            "320:\ttest: 0.7817212\tbest: 0.7817212 (320)\ttotal: 10m 49s\tremaining: 22m 53s\n",
            "321:\ttotal: 10m 51s\tremaining: 22m 52s\n",
            "322:\ttotal: 10m 54s\tremaining: 22m 51s\n",
            "323:\ttotal: 10m 56s\tremaining: 22m 50s\n",
            "324:\ttotal: 10m 58s\tremaining: 22m 47s\n",
            "325:\ttest: 0.7818562\tbest: 0.7818562 (325)\ttotal: 11m\tremaining: 22m 44s\n",
            "326:\ttotal: 11m 2s\tremaining: 22m 43s\n",
            "327:\ttotal: 11m 4s\tremaining: 22m 40s\n",
            "328:\ttotal: 11m 5s\tremaining: 22m 38s\n",
            "329:\ttotal: 11m 7s\tremaining: 22m 35s\n",
            "330:\ttest: 0.7819987\tbest: 0.7819987 (330)\ttotal: 11m 9s\tremaining: 22m 32s\n",
            "331:\ttotal: 11m 10s\tremaining: 22m 29s\n",
            "332:\ttotal: 11m 12s\tremaining: 22m 27s\n",
            "333:\ttotal: 11m 14s\tremaining: 22m 25s\n",
            "334:\ttotal: 11m 16s\tremaining: 22m 22s\n",
            "335:\ttest: 0.7821441\tbest: 0.7821441 (335)\ttotal: 11m 18s\tremaining: 22m 20s\n",
            "336:\ttotal: 11m 20s\tremaining: 22m 17s\n",
            "337:\ttotal: 11m 21s\tremaining: 22m 15s\n",
            "338:\ttotal: 11m 24s\tremaining: 22m 14s\n",
            "339:\ttotal: 11m 26s\tremaining: 22m 13s\n",
            "340:\ttest: 0.7822798\tbest: 0.7822798 (340)\ttotal: 11m 28s\tremaining: 22m 10s\n",
            "341:\ttotal: 11m 30s\tremaining: 22m 8s\n",
            "342:\ttotal: 11m 32s\tremaining: 22m 6s\n",
            "343:\ttotal: 11m 34s\tremaining: 22m 4s\n",
            "344:\ttotal: 11m 35s\tremaining: 22m 1s\n",
            "345:\ttest: 0.7824047\tbest: 0.7824047 (345)\ttotal: 11m 37s\tremaining: 21m 58s\n",
            "346:\ttotal: 11m 40s\tremaining: 21m 57s\n",
            "347:\ttotal: 11m 42s\tremaining: 21m 56s\n",
            "348:\ttotal: 11m 44s\tremaining: 21m 54s\n",
            "349:\ttotal: 11m 46s\tremaining: 21m 52s\n",
            "350:\ttest: 0.7825090\tbest: 0.7825090 (350)\ttotal: 11m 48s\tremaining: 21m 49s\n",
            "351:\ttotal: 11m 50s\tremaining: 21m 48s\n",
            "352:\ttotal: 11m 52s\tremaining: 21m 45s\n",
            "353:\ttotal: 11m 53s\tremaining: 21m 42s\n",
            "354:\ttotal: 11m 56s\tremaining: 21m 41s\n",
            "355:\ttest: 0.7826232\tbest: 0.7826232 (355)\ttotal: 11m 58s\tremaining: 21m 39s\n",
            "356:\ttotal: 11m 59s\tremaining: 21m 36s\n",
            "357:\ttotal: 12m 1s\tremaining: 21m 34s\n",
            "358:\ttotal: 12m 3s\tremaining: 21m 32s\n",
            "359:\ttotal: 12m 5s\tremaining: 21m 29s\n",
            "360:\ttest: 0.7827587\tbest: 0.7827587 (360)\ttotal: 12m 7s\tremaining: 21m 28s\n",
            "361:\ttotal: 12m 9s\tremaining: 21m 25s\n",
            "362:\ttotal: 12m 11s\tremaining: 21m 24s\n",
            "363:\ttotal: 12m 13s\tremaining: 21m 21s\n",
            "364:\ttotal: 12m 15s\tremaining: 21m 18s\n",
            "365:\ttest: 0.7828795\tbest: 0.7828795 (365)\ttotal: 12m 17s\tremaining: 21m 16s\n",
            "366:\ttotal: 12m 18s\tremaining: 21m 14s\n",
            "367:\ttotal: 12m 21s\tremaining: 21m 12s\n",
            "368:\ttotal: 12m 23s\tremaining: 21m 10s\n",
            "369:\ttotal: 12m 25s\tremaining: 21m 9s\n",
            "370:\ttest: 0.7829981\tbest: 0.7829981 (370)\ttotal: 12m 27s\tremaining: 21m 7s\n",
            "371:\ttotal: 12m 29s\tremaining: 21m 5s\n",
            "372:\ttotal: 12m 31s\tremaining: 21m 2s\n",
            "373:\ttotal: 12m 32s\tremaining: 21m\n",
            "374:\ttotal: 12m 34s\tremaining: 20m 58s\n",
            "375:\ttest: 0.7830704\tbest: 0.7830704 (375)\ttotal: 12m 37s\tremaining: 20m 56s\n",
            "376:\ttotal: 12m 38s\tremaining: 20m 53s\n",
            "377:\ttotal: 12m 41s\tremaining: 20m 52s\n",
            "378:\ttotal: 12m 43s\tremaining: 20m 51s\n",
            "379:\ttotal: 12m 45s\tremaining: 20m 49s\n",
            "380:\ttest: 0.7831752\tbest: 0.7831752 (380)\ttotal: 12m 48s\tremaining: 20m 48s\n",
            "381:\ttotal: 12m 50s\tremaining: 20m 46s\n",
            "382:\ttotal: 12m 52s\tremaining: 20m 43s\n",
            "383:\ttotal: 12m 53s\tremaining: 20m 41s\n",
            "384:\ttotal: 12m 55s\tremaining: 20m 38s\n",
            "385:\ttest: 0.7832984\tbest: 0.7832984 (385)\ttotal: 12m 57s\tremaining: 20m 36s\n",
            "386:\ttotal: 12m 59s\tremaining: 20m 34s\n",
            "387:\ttotal: 13m 1s\tremaining: 20m 33s\n",
            "388:\ttotal: 13m 3s\tremaining: 20m 30s\n",
            "389:\ttotal: 13m 5s\tremaining: 20m 28s\n",
            "390:\ttest: 0.7834050\tbest: 0.7834050 (390)\ttotal: 13m 7s\tremaining: 20m 26s\n",
            "391:\ttotal: 13m 9s\tremaining: 20m 24s\n",
            "392:\ttotal: 13m 11s\tremaining: 20m 21s\n",
            "393:\ttotal: 13m 13s\tremaining: 20m 20s\n",
            "394:\ttotal: 13m 15s\tremaining: 20m 17s\n",
            "395:\ttest: 0.7835177\tbest: 0.7835177 (395)\ttotal: 13m 16s\tremaining: 20m 15s\n",
            "396:\ttotal: 13m 18s\tremaining: 20m 12s\n",
            "397:\ttotal: 13m 19s\tremaining: 20m 9s\n",
            "398:\ttotal: 13m 22s\tremaining: 20m 8s\n",
            "399:\ttotal: 13m 24s\tremaining: 20m 6s\n",
            "400:\ttest: 0.7836352\tbest: 0.7836352 (400)\ttotal: 13m 25s\tremaining: 20m 3s\n",
            "401:\ttotal: 13m 27s\tremaining: 20m 1s\n",
            "402:\ttotal: 13m 29s\tremaining: 19m 59s\n",
            "403:\ttotal: 13m 31s\tremaining: 19m 57s\n",
            "404:\ttotal: 13m 33s\tremaining: 19m 54s\n",
            "405:\ttest: 0.7837277\tbest: 0.7837277 (405)\ttotal: 13m 35s\tremaining: 19m 52s\n",
            "406:\ttotal: 13m 37s\tremaining: 19m 51s\n",
            "407:\ttotal: 13m 39s\tremaining: 19m 49s\n",
            "408:\ttotal: 13m 42s\tremaining: 19m 48s\n",
            "409:\ttotal: 13m 44s\tremaining: 19m 46s\n",
            "410:\ttest: 0.7838334\tbest: 0.7838334 (410)\ttotal: 13m 46s\tremaining: 19m 45s\n",
            "411:\ttotal: 13m 49s\tremaining: 19m 43s\n",
            "412:\ttotal: 13m 51s\tremaining: 19m 41s\n",
            "413:\ttotal: 13m 53s\tremaining: 19m 39s\n",
            "414:\ttotal: 13m 54s\tremaining: 19m 36s\n",
            "415:\ttest: 0.7839170\tbest: 0.7839170 (415)\ttotal: 13m 56s\tremaining: 19m 34s\n",
            "416:\ttotal: 13m 58s\tremaining: 19m 32s\n",
            "417:\ttotal: 14m\tremaining: 19m 29s\n",
            "418:\ttotal: 14m 1s\tremaining: 19m 27s\n",
            "419:\ttotal: 14m 3s\tremaining: 19m 24s\n",
            "420:\ttest: 0.7840163\tbest: 0.7840163 (420)\ttotal: 14m 4s\tremaining: 19m 21s\n",
            "421:\ttotal: 14m 7s\tremaining: 19m 20s\n",
            "422:\ttotal: 14m 8s\tremaining: 19m 17s\n",
            "423:\ttotal: 14m 10s\tremaining: 19m 15s\n",
            "424:\ttotal: 14m 12s\tremaining: 19m 13s\n",
            "425:\ttest: 0.7841349\tbest: 0.7841349 (425)\ttotal: 14m 14s\tremaining: 19m 11s\n",
            "426:\ttotal: 14m 17s\tremaining: 19m 10s\n",
            "427:\ttotal: 14m 18s\tremaining: 19m 7s\n",
            "428:\ttotal: 14m 20s\tremaining: 19m 5s\n",
            "429:\ttotal: 14m 22s\tremaining: 19m 2s\n",
            "430:\ttest: 0.7842528\tbest: 0.7842528 (430)\ttotal: 14m 23s\tremaining: 19m\n",
            "431:\ttotal: 14m 25s\tremaining: 18m 58s\n",
            "432:\ttotal: 14m 27s\tremaining: 18m 55s\n",
            "433:\ttotal: 14m 29s\tremaining: 18m 54s\n",
            "434:\ttotal: 14m 32s\tremaining: 18m 53s\n",
            "435:\ttest: 0.7843094\tbest: 0.7843123 (434)\ttotal: 14m 34s\tremaining: 18m 50s\n",
            "436:\ttotal: 14m 36s\tremaining: 18m 48s\n",
            "437:\ttotal: 14m 37s\tremaining: 18m 46s\n",
            "438:\ttotal: 14m 39s\tremaining: 18m 43s\n",
            "439:\ttotal: 14m 42s\tremaining: 18m 42s\n",
            "440:\ttest: 0.7844049\tbest: 0.7844049 (440)\ttotal: 14m 44s\tremaining: 18m 41s\n",
            "441:\ttotal: 14m 46s\tremaining: 18m 39s\n",
            "442:\ttotal: 14m 48s\tremaining: 18m 37s\n",
            "443:\ttotal: 14m 50s\tremaining: 18m 35s\n",
            "444:\ttotal: 14m 52s\tremaining: 18m 33s\n",
            "445:\ttest: 0.7844505\tbest: 0.7844505 (445)\ttotal: 14m 54s\tremaining: 18m 31s\n",
            "446:\ttotal: 14m 56s\tremaining: 18m 29s\n",
            "447:\ttotal: 14m 59s\tremaining: 18m 27s\n",
            "448:\ttotal: 15m 1s\tremaining: 18m 25s\n",
            "449:\ttotal: 15m 2s\tremaining: 18m 23s\n",
            "450:\ttest: 0.7845578\tbest: 0.7845578 (450)\ttotal: 15m 5s\tremaining: 18m 21s\n",
            "451:\ttotal: 15m 6s\tremaining: 18m 19s\n",
            "452:\ttotal: 15m 8s\tremaining: 18m 17s\n",
            "453:\ttotal: 15m 10s\tremaining: 18m 14s\n",
            "454:\ttotal: 15m 12s\tremaining: 18m 12s\n",
            "455:\ttest: 0.7846455\tbest: 0.7846455 (455)\ttotal: 15m 14s\tremaining: 18m 10s\n",
            "456:\ttotal: 15m 16s\tremaining: 18m 9s\n",
            "457:\ttotal: 15m 18s\tremaining: 18m 7s\n",
            "458:\ttotal: 15m 20s\tremaining: 18m 5s\n",
            "459:\ttotal: 15m 22s\tremaining: 18m 2s\n",
            "460:\ttest: 0.7847288\tbest: 0.7847288 (460)\ttotal: 15m 24s\tremaining: 18m\n",
            "461:\ttotal: 15m 26s\tremaining: 17m 58s\n",
            "462:\ttotal: 15m 27s\tremaining: 17m 56s\n",
            "463:\ttotal: 15m 29s\tremaining: 17m 53s\n",
            "464:\ttotal: 15m 31s\tremaining: 17m 51s\n",
            "465:\ttest: 0.7848091\tbest: 0.7848091 (465)\ttotal: 15m 33s\tremaining: 17m 49s\n",
            "466:\ttotal: 15m 35s\tremaining: 17m 48s\n",
            "467:\ttotal: 15m 37s\tremaining: 17m 45s\n",
            "468:\ttotal: 15m 39s\tremaining: 17m 43s\n",
            "469:\ttotal: 15m 41s\tremaining: 17m 41s\n",
            "470:\ttest: 0.7849008\tbest: 0.7849008 (470)\ttotal: 15m 42s\tremaining: 17m 38s\n",
            "471:\ttotal: 15m 44s\tremaining: 17m 37s\n",
            "472:\ttotal: 15m 47s\tremaining: 17m 35s\n",
            "473:\ttotal: 15m 48s\tremaining: 17m 32s\n",
            "474:\ttotal: 15m 51s\tremaining: 17m 31s\n",
            "475:\ttest: 0.7849717\tbest: 0.7849717 (475)\ttotal: 15m 53s\tremaining: 17m 29s\n",
            "476:\ttotal: 15m 55s\tremaining: 17m 27s\n",
            "477:\ttotal: 15m 57s\tremaining: 17m 25s\n",
            "478:\ttotal: 15m 59s\tremaining: 17m 23s\n",
            "479:\ttotal: 16m 1s\tremaining: 17m 21s\n",
            "480:\ttest: 0.7850994\tbest: 0.7850994 (480)\ttotal: 16m 2s\tremaining: 17m 18s\n",
            "481:\ttotal: 16m 5s\tremaining: 17m 17s\n",
            "482:\ttotal: 16m 6s\tremaining: 17m 14s\n",
            "483:\ttotal: 16m 9s\tremaining: 17m 13s\n",
            "484:\ttotal: 16m 11s\tremaining: 17m 11s\n",
            "485:\ttest: 0.7851720\tbest: 0.7851720 (485)\ttotal: 16m 13s\tremaining: 17m 9s\n",
            "486:\ttotal: 16m 15s\tremaining: 17m 7s\n",
            "487:\ttotal: 16m 17s\tremaining: 17m 5s\n",
            "488:\ttotal: 16m 19s\tremaining: 17m 3s\n",
            "489:\ttotal: 16m 20s\tremaining: 17m 1s\n",
            "490:\ttest: 0.7852466\tbest: 0.7852466 (490)\ttotal: 16m 23s\tremaining: 16m 59s\n",
            "491:\ttotal: 16m 25s\tremaining: 16m 57s\n",
            "492:\ttotal: 16m 26s\tremaining: 16m 54s\n",
            "493:\ttotal: 16m 28s\tremaining: 16m 52s\n",
            "494:\ttotal: 16m 30s\tremaining: 16m 50s\n",
            "495:\ttest: 0.7853216\tbest: 0.7853216 (495)\ttotal: 16m 32s\tremaining: 16m 48s\n",
            "496:\ttotal: 16m 34s\tremaining: 16m 46s\n",
            "497:\ttotal: 16m 37s\tremaining: 16m 45s\n",
            "498:\ttotal: 16m 39s\tremaining: 16m 43s\n",
            "499:\ttotal: 16m 41s\tremaining: 16m 41s\n",
            "500:\ttest: 0.7854028\tbest: 0.7854028 (500)\ttotal: 16m 43s\tremaining: 16m 39s\n",
            "501:\ttotal: 16m 45s\tremaining: 16m 37s\n",
            "502:\ttotal: 16m 47s\tremaining: 16m 35s\n",
            "503:\ttotal: 16m 49s\tremaining: 16m 33s\n",
            "504:\ttotal: 16m 51s\tremaining: 16m 31s\n",
            "505:\ttest: 0.7854996\tbest: 0.7854996 (505)\ttotal: 16m 52s\tremaining: 16m 28s\n",
            "506:\ttotal: 16m 55s\tremaining: 16m 27s\n",
            "507:\ttotal: 16m 57s\tremaining: 16m 25s\n",
            "508:\ttotal: 16m 59s\tremaining: 16m 23s\n",
            "509:\ttotal: 17m 1s\tremaining: 16m 21s\n",
            "510:\ttest: 0.7855840\tbest: 0.7855840 (510)\ttotal: 17m 3s\tremaining: 16m 19s\n",
            "511:\ttotal: 17m 5s\tremaining: 16m 17s\n",
            "512:\ttotal: 17m 7s\tremaining: 16m 15s\n",
            "513:\ttotal: 17m 10s\tremaining: 16m 14s\n",
            "514:\ttotal: 17m 12s\tremaining: 16m 11s\n",
            "515:\ttest: 0.7856644\tbest: 0.7856644 (515)\ttotal: 17m 13s\tremaining: 16m 9s\n",
            "516:\ttotal: 17m 15s\tremaining: 16m 7s\n",
            "517:\ttotal: 17m 17s\tremaining: 16m 5s\n",
            "518:\ttotal: 17m 19s\tremaining: 16m 3s\n",
            "519:\ttotal: 17m 21s\tremaining: 16m 1s\n",
            "520:\ttest: 0.7857153\tbest: 0.7857153 (520)\ttotal: 17m 23s\tremaining: 15m 59s\n",
            "521:\ttotal: 17m 24s\tremaining: 15m 56s\n",
            "522:\ttotal: 17m 26s\tremaining: 15m 54s\n",
            "523:\ttotal: 17m 29s\tremaining: 15m 52s\n",
            "524:\ttotal: 17m 31s\tremaining: 15m 51s\n",
            "525:\ttest: 0.7857896\tbest: 0.7857896 (525)\ttotal: 17m 33s\tremaining: 15m 49s\n",
            "526:\ttotal: 17m 35s\tremaining: 15m 47s\n",
            "527:\ttotal: 17m 36s\tremaining: 15m 44s\n",
            "528:\ttotal: 17m 38s\tremaining: 15m 42s\n",
            "529:\ttotal: 17m 40s\tremaining: 15m 40s\n",
            "530:\ttest: 0.7858511\tbest: 0.7858511 (530)\ttotal: 17m 42s\tremaining: 15m 38s\n",
            "531:\ttotal: 17m 43s\tremaining: 15m 35s\n",
            "532:\ttotal: 17m 45s\tremaining: 15m 33s\n",
            "533:\ttotal: 17m 47s\tremaining: 15m 31s\n",
            "534:\ttotal: 17m 49s\tremaining: 15m 29s\n",
            "535:\ttest: 0.7859452\tbest: 0.7859452 (535)\ttotal: 17m 51s\tremaining: 15m 27s\n",
            "536:\ttotal: 17m 53s\tremaining: 15m 25s\n",
            "537:\ttotal: 17m 55s\tremaining: 15m 23s\n",
            "538:\ttotal: 17m 57s\tremaining: 15m 21s\n",
            "539:\ttotal: 17m 59s\tremaining: 15m 19s\n",
            "540:\ttest: 0.7860169\tbest: 0.7860169 (540)\ttotal: 18m 1s\tremaining: 15m 17s\n",
            "541:\ttotal: 18m 3s\tremaining: 15m 15s\n",
            "542:\ttotal: 18m 5s\tremaining: 15m 13s\n",
            "543:\ttotal: 18m 7s\tremaining: 15m 11s\n",
            "544:\ttotal: 18m 8s\tremaining: 15m 8s\n",
            "545:\ttest: 0.7860934\tbest: 0.7860934 (545)\ttotal: 18m 10s\tremaining: 15m 7s\n",
            "546:\ttotal: 18m 13s\tremaining: 15m 5s\n",
            "547:\ttotal: 18m 15s\tremaining: 15m 3s\n",
            "548:\ttotal: 18m 17s\tremaining: 15m 1s\n",
            "549:\ttotal: 18m 19s\tremaining: 14m 59s\n",
            "550:\ttest: 0.7861544\tbest: 0.7861544 (550)\ttotal: 18m 21s\tremaining: 14m 57s\n",
            "551:\ttotal: 18m 24s\tremaining: 14m 56s\n",
            "552:\ttotal: 18m 25s\tremaining: 14m 53s\n",
            "553:\ttotal: 18m 27s\tremaining: 14m 51s\n",
            "554:\ttotal: 18m 29s\tremaining: 14m 49s\n",
            "555:\ttest: 0.7862485\tbest: 0.7862485 (555)\ttotal: 18m 31s\tremaining: 14m 47s\n",
            "556:\ttotal: 18m 34s\tremaining: 14m 46s\n",
            "557:\ttotal: 18m 36s\tremaining: 14m 44s\n",
            "558:\ttotal: 18m 38s\tremaining: 14m 42s\n",
            "559:\ttotal: 18m 40s\tremaining: 14m 40s\n",
            "560:\ttest: 0.7863097\tbest: 0.7863097 (560)\ttotal: 18m 42s\tremaining: 14m 38s\n",
            "561:\ttotal: 18m 43s\tremaining: 14m 35s\n",
            "562:\ttotal: 18m 45s\tremaining: 14m 33s\n",
            "563:\ttotal: 18m 48s\tremaining: 14m 32s\n",
            "564:\ttotal: 18m 49s\tremaining: 14m 29s\n",
            "565:\ttest: 0.7863779\tbest: 0.7863779 (565)\ttotal: 18m 51s\tremaining: 14m 27s\n",
            "566:\ttotal: 18m 53s\tremaining: 14m 25s\n",
            "567:\ttotal: 18m 55s\tremaining: 14m 23s\n",
            "568:\ttotal: 18m 56s\tremaining: 14m 21s\n",
            "569:\ttotal: 18m 59s\tremaining: 14m 19s\n",
            "570:\ttest: 0.7864361\tbest: 0.7864361 (570)\ttotal: 19m 1s\tremaining: 14m 17s\n",
            "571:\ttotal: 19m 4s\tremaining: 14m 16s\n",
            "572:\ttotal: 19m 5s\tremaining: 14m 13s\n",
            "573:\ttotal: 19m 8s\tremaining: 14m 12s\n",
            "574:\ttotal: 19m 9s\tremaining: 14m 9s\n",
            "575:\ttest: 0.7864962\tbest: 0.7864962 (575)\ttotal: 19m 11s\tremaining: 14m 7s\n",
            "576:\ttotal: 19m 13s\tremaining: 14m 5s\n",
            "577:\ttotal: 19m 15s\tremaining: 14m 3s\n",
            "578:\ttotal: 19m 17s\tremaining: 14m 1s\n",
            "579:\ttotal: 19m 19s\tremaining: 13m 59s\n",
            "580:\ttest: 0.7865721\tbest: 0.7865721 (580)\ttotal: 19m 22s\tremaining: 13m 58s\n",
            "581:\ttotal: 19m 23s\tremaining: 13m 55s\n",
            "582:\ttotal: 19m 25s\tremaining: 13m 53s\n",
            "583:\ttotal: 19m 27s\tremaining: 13m 51s\n",
            "584:\ttotal: 19m 29s\tremaining: 13m 49s\n",
            "585:\ttest: 0.7866429\tbest: 0.7866429 (585)\ttotal: 19m 30s\tremaining: 13m 47s\n",
            "586:\ttotal: 19m 33s\tremaining: 13m 45s\n",
            "587:\ttotal: 19m 34s\tremaining: 13m 43s\n",
            "588:\ttotal: 19m 37s\tremaining: 13m 41s\n",
            "589:\ttotal: 19m 40s\tremaining: 13m 40s\n",
            "590:\ttest: 0.7866969\tbest: 0.7866969 (590)\ttotal: 19m 41s\tremaining: 13m 37s\n",
            "591:\ttotal: 19m 43s\tremaining: 13m 35s\n",
            "592:\ttotal: 19m 45s\tremaining: 13m 33s\n",
            "593:\ttotal: 19m 47s\tremaining: 13m 31s\n",
            "594:\ttotal: 19m 49s\tremaining: 13m 29s\n",
            "595:\ttest: 0.7867422\tbest: 0.7867422 (595)\ttotal: 19m 50s\tremaining: 13m 27s\n",
            "596:\ttotal: 19m 52s\tremaining: 13m 24s\n",
            "597:\ttotal: 19m 54s\tremaining: 13m 22s\n",
            "598:\ttotal: 19m 56s\tremaining: 13m 20s\n",
            "599:\ttotal: 19m 58s\tremaining: 13m 19s\n",
            "600:\ttest: 0.7867848\tbest: 0.7867848 (600)\ttotal: 20m 1s\tremaining: 13m 17s\n",
            "601:\ttotal: 20m 2s\tremaining: 13m 15s\n",
            "602:\ttotal: 20m 4s\tremaining: 13m 13s\n",
            "603:\ttotal: 20m 7s\tremaining: 13m 11s\n",
            "604:\ttotal: 20m 9s\tremaining: 13m 9s\n",
            "605:\ttest: 0.7868549\tbest: 0.7868549 (605)\ttotal: 20m 12s\tremaining: 13m 8s\n",
            "606:\ttotal: 20m 14s\tremaining: 13m 6s\n",
            "607:\ttotal: 20m 15s\tremaining: 13m 3s\n",
            "608:\ttotal: 20m 18s\tremaining: 13m 2s\n",
            "609:\ttotal: 20m 20s\tremaining: 13m\n",
            "610:\ttest: 0.7868986\tbest: 0.7868986 (610)\ttotal: 20m 22s\tremaining: 12m 58s\n",
            "611:\ttotal: 20m 24s\tremaining: 12m 56s\n",
            "612:\ttotal: 20m 26s\tremaining: 12m 54s\n",
            "613:\ttotal: 20m 28s\tremaining: 12m 52s\n",
            "614:\ttotal: 20m 30s\tremaining: 12m 50s\n",
            "615:\ttest: 0.7869477\tbest: 0.7869477 (615)\ttotal: 20m 32s\tremaining: 12m 48s\n",
            "616:\ttotal: 20m 33s\tremaining: 12m 45s\n",
            "617:\ttotal: 20m 35s\tremaining: 12m 43s\n",
            "618:\ttotal: 20m 38s\tremaining: 12m 42s\n",
            "619:\ttotal: 20m 40s\tremaining: 12m 40s\n",
            "620:\ttest: 0.7870012\tbest: 0.7870012 (620)\ttotal: 20m 42s\tremaining: 12m 38s\n",
            "621:\ttotal: 20m 44s\tremaining: 12m 36s\n",
            "622:\ttotal: 20m 46s\tremaining: 12m 34s\n",
            "623:\ttotal: 20m 47s\tremaining: 12m 31s\n",
            "624:\ttotal: 20m 50s\tremaining: 12m 30s\n",
            "625:\ttest: 0.7870561\tbest: 0.7870561 (625)\ttotal: 20m 52s\tremaining: 12m 28s\n",
            "626:\ttotal: 20m 54s\tremaining: 12m 26s\n",
            "627:\ttotal: 20m 56s\tremaining: 12m 24s\n",
            "628:\ttotal: 20m 57s\tremaining: 12m 21s\n",
            "629:\ttotal: 21m\tremaining: 12m 20s\n",
            "630:\ttest: 0.7871308\tbest: 0.7871308 (630)\ttotal: 21m 1s\tremaining: 12m 17s\n",
            "631:\ttotal: 21m 3s\tremaining: 12m 15s\n",
            "632:\ttotal: 21m 5s\tremaining: 12m 13s\n",
            "633:\ttotal: 21m 7s\tremaining: 12m 11s\n",
            "634:\ttotal: 21m 8s\tremaining: 12m 9s\n",
            "635:\ttest: 0.7872036\tbest: 0.7872036 (635)\ttotal: 21m 11s\tremaining: 12m 7s\n",
            "636:\ttotal: 21m 13s\tremaining: 12m 5s\n",
            "637:\ttotal: 21m 14s\tremaining: 12m 3s\n",
            "638:\ttotal: 21m 16s\tremaining: 12m 1s\n",
            "639:\ttotal: 21m 18s\tremaining: 11m 59s\n",
            "640:\ttest: 0.7872779\tbest: 0.7872779 (640)\ttotal: 21m 21s\tremaining: 11m 57s\n",
            "641:\ttotal: 21m 23s\tremaining: 11m 55s\n",
            "642:\ttotal: 21m 25s\tremaining: 11m 53s\n",
            "643:\ttotal: 21m 27s\tremaining: 11m 51s\n",
            "644:\ttotal: 21m 29s\tremaining: 11m 49s\n",
            "645:\ttest: 0.7873285\tbest: 0.7873285 (645)\ttotal: 21m 31s\tremaining: 11m 47s\n",
            "646:\ttotal: 21m 32s\tremaining: 11m 45s\n",
            "647:\ttotal: 21m 34s\tremaining: 11m 43s\n",
            "648:\ttotal: 21m 36s\tremaining: 11m 41s\n",
            "649:\ttotal: 21m 38s\tremaining: 11m 38s\n",
            "650:\ttest: 0.7873648\tbest: 0.7873648 (650)\ttotal: 21m 40s\tremaining: 11m 37s\n",
            "651:\ttotal: 21m 42s\tremaining: 11m 35s\n",
            "652:\ttotal: 21m 44s\tremaining: 11m 33s\n",
            "653:\ttotal: 21m 46s\tremaining: 11m 31s\n",
            "654:\ttotal: 21m 48s\tremaining: 11m 29s\n",
            "655:\ttest: 0.7874226\tbest: 0.7874226 (655)\ttotal: 21m 50s\tremaining: 11m 27s\n",
            "656:\ttotal: 21m 52s\tremaining: 11m 25s\n",
            "657:\ttotal: 21m 54s\tremaining: 11m 23s\n",
            "658:\ttotal: 21m 56s\tremaining: 11m 21s\n",
            "659:\ttotal: 21m 59s\tremaining: 11m 19s\n",
            "660:\ttest: 0.7874819\tbest: 0.7874819 (660)\ttotal: 22m\tremaining: 11m 17s\n",
            "661:\ttotal: 22m 2s\tremaining: 11m 15s\n",
            "662:\ttotal: 22m 4s\tremaining: 11m 12s\n",
            "663:\ttotal: 22m 5s\tremaining: 11m 10s\n",
            "664:\ttotal: 22m 7s\tremaining: 11m 8s\n",
            "665:\ttest: 0.7875513\tbest: 0.7875513 (665)\ttotal: 22m 9s\tremaining: 11m 6s\n",
            "666:\ttotal: 22m 12s\tremaining: 11m 5s\n",
            "667:\ttotal: 22m 14s\tremaining: 11m 3s\n",
            "668:\ttotal: 22m 16s\tremaining: 11m 1s\n",
            "669:\ttotal: 22m 18s\tremaining: 10m 59s\n",
            "670:\ttest: 0.7875740\tbest: 0.7875740 (670)\ttotal: 22m 21s\tremaining: 10m 57s\n",
            "671:\ttotal: 22m 23s\tremaining: 10m 55s\n",
            "672:\ttotal: 22m 24s\tremaining: 10m 53s\n",
            "673:\ttotal: 22m 26s\tremaining: 10m 51s\n",
            "674:\ttotal: 22m 29s\tremaining: 10m 49s\n",
            "675:\ttest: 0.7876290\tbest: 0.7876290 (675)\ttotal: 22m 31s\tremaining: 10m 47s\n",
            "676:\ttotal: 22m 32s\tremaining: 10m 45s\n",
            "677:\ttotal: 22m 35s\tremaining: 10m 43s\n",
            "678:\ttotal: 22m 37s\tremaining: 10m 41s\n",
            "679:\ttotal: 22m 39s\tremaining: 10m 39s\n",
            "680:\ttest: 0.7876823\tbest: 0.7876823 (680)\ttotal: 22m 41s\tremaining: 10m 37s\n",
            "681:\ttotal: 22m 43s\tremaining: 10m 35s\n",
            "682:\ttotal: 22m 45s\tremaining: 10m 33s\n",
            "683:\ttotal: 22m 48s\tremaining: 10m 32s\n",
            "684:\ttotal: 22m 50s\tremaining: 10m 30s\n",
            "685:\ttest: 0.7877356\tbest: 0.7877356 (685)\ttotal: 22m 52s\tremaining: 10m 28s\n",
            "686:\ttotal: 22m 54s\tremaining: 10m 26s\n",
            "687:\ttotal: 22m 56s\tremaining: 10m 24s\n",
            "688:\ttotal: 22m 58s\tremaining: 10m 22s\n",
            "689:\ttotal: 23m\tremaining: 10m 20s\n",
            "690:\ttest: 0.7877763\tbest: 0.7877763 (690)\ttotal: 23m 3s\tremaining: 10m 18s\n",
            "691:\ttotal: 23m 5s\tremaining: 10m 16s\n",
            "692:\ttotal: 23m 7s\tremaining: 10m 14s\n",
            "693:\ttotal: 23m 8s\tremaining: 10m 12s\n",
            "694:\ttotal: 23m 10s\tremaining: 10m 10s\n",
            "695:\ttest: 0.7878207\tbest: 0.7878207 (695)\ttotal: 23m 13s\tremaining: 10m 8s\n",
            "696:\ttotal: 23m 15s\tremaining: 10m 6s\n",
            "697:\ttotal: 23m 16s\tremaining: 10m 4s\n",
            "698:\ttotal: 23m 18s\tremaining: 10m 2s\n",
            "699:\ttotal: 23m 20s\tremaining: 10m\n",
            "700:\ttest: 0.7878750\tbest: 0.7878750 (700)\ttotal: 23m 22s\tremaining: 9m 58s\n",
            "701:\ttotal: 23m 24s\tremaining: 9m 56s\n",
            "702:\ttotal: 23m 26s\tremaining: 9m 54s\n",
            "703:\ttotal: 23m 28s\tremaining: 9m 52s\n",
            "704:\ttotal: 23m 31s\tremaining: 9m 50s\n",
            "705:\ttest: 0.7879219\tbest: 0.7879219 (705)\ttotal: 23m 33s\tremaining: 9m 48s\n",
            "706:\ttotal: 23m 35s\tremaining: 9m 46s\n",
            "707:\ttotal: 23m 37s\tremaining: 9m 44s\n",
            "708:\ttotal: 23m 39s\tremaining: 9m 42s\n",
            "709:\ttotal: 23m 41s\tremaining: 9m 40s\n",
            "710:\ttest: 0.7879860\tbest: 0.7879860 (710)\ttotal: 23m 43s\tremaining: 9m 38s\n",
            "711:\ttotal: 23m 44s\tremaining: 9m 36s\n",
            "712:\ttotal: 23m 46s\tremaining: 9m 34s\n",
            "713:\ttotal: 23m 49s\tremaining: 9m 32s\n",
            "714:\ttotal: 23m 51s\tremaining: 9m 30s\n",
            "715:\ttest: 0.7880355\tbest: 0.7880355 (715)\ttotal: 23m 54s\tremaining: 9m 28s\n",
            "716:\ttotal: 23m 56s\tremaining: 9m 26s\n",
            "717:\ttotal: 23m 58s\tremaining: 9m 25s\n",
            "718:\ttotal: 24m\tremaining: 9m 23s\n",
            "719:\ttotal: 24m 3s\tremaining: 9m 21s\n",
            "720:\ttest: 0.7880774\tbest: 0.7880774 (720)\ttotal: 24m 4s\tremaining: 9m 19s\n",
            "721:\ttotal: 24m 6s\tremaining: 9m 16s\n",
            "722:\ttotal: 24m 8s\tremaining: 9m 14s\n",
            "723:\ttotal: 24m 10s\tremaining: 9m 12s\n",
            "724:\ttotal: 24m 11s\tremaining: 9m 10s\n",
            "725:\ttest: 0.7881381\tbest: 0.7881381 (725)\ttotal: 24m 13s\tremaining: 9m 8s\n",
            "726:\ttotal: 24m 15s\tremaining: 9m 6s\n",
            "727:\ttotal: 24m 18s\tremaining: 9m 4s\n",
            "728:\ttotal: 24m 20s\tremaining: 9m 2s\n",
            "729:\ttotal: 24m 21s\tremaining: 9m\n",
            "730:\ttest: 0.7881861\tbest: 0.7881861 (730)\ttotal: 24m 23s\tremaining: 8m 58s\n",
            "731:\ttotal: 24m 25s\tremaining: 8m 56s\n",
            "732:\ttotal: 24m 27s\tremaining: 8m 54s\n",
            "733:\ttotal: 24m 28s\tremaining: 8m 52s\n",
            "734:\ttotal: 24m 30s\tremaining: 8m 50s\n",
            "735:\ttest: 0.7882343\tbest: 0.7882343 (735)\ttotal: 24m 33s\tremaining: 8m 48s\n",
            "736:\ttotal: 24m 35s\tremaining: 8m 46s\n",
            "737:\ttotal: 24m 37s\tremaining: 8m 44s\n",
            "738:\ttotal: 24m 39s\tremaining: 8m 42s\n",
            "739:\ttotal: 24m 42s\tremaining: 8m 40s\n",
            "740:\ttest: 0.7882786\tbest: 0.7882786 (740)\ttotal: 24m 44s\tremaining: 8m 38s\n",
            "741:\ttotal: 24m 46s\tremaining: 8m 36s\n",
            "742:\ttotal: 24m 48s\tremaining: 8m 34s\n",
            "743:\ttotal: 24m 49s\tremaining: 8m 32s\n",
            "744:\ttotal: 24m 52s\tremaining: 8m 30s\n",
            "745:\ttest: 0.7883398\tbest: 0.7883398 (745)\ttotal: 24m 53s\tremaining: 8m 28s\n",
            "746:\ttotal: 24m 56s\tremaining: 8m 26s\n",
            "747:\ttotal: 24m 58s\tremaining: 8m 24s\n",
            "748:\ttotal: 25m 1s\tremaining: 8m 23s\n",
            "749:\ttotal: 25m 3s\tremaining: 8m 21s\n",
            "750:\ttest: 0.7883869\tbest: 0.7883869 (750)\ttotal: 25m 5s\tremaining: 8m 19s\n",
            "751:\ttotal: 25m 7s\tremaining: 8m 17s\n",
            "752:\ttotal: 25m 9s\tremaining: 8m 15s\n",
            "753:\ttotal: 25m 11s\tremaining: 8m 13s\n",
            "754:\ttotal: 25m 12s\tremaining: 8m 10s\n",
            "755:\ttest: 0.7884558\tbest: 0.7884558 (755)\ttotal: 25m 14s\tremaining: 8m 8s\n",
            "756:\ttotal: 25m 16s\tremaining: 8m 6s\n",
            "757:\ttotal: 25m 19s\tremaining: 8m 4s\n",
            "758:\ttotal: 25m 21s\tremaining: 8m 3s\n",
            "759:\ttotal: 25m 23s\tremaining: 8m\n",
            "760:\ttest: 0.7885082\tbest: 0.7885082 (760)\ttotal: 25m 24s\tremaining: 7m 58s\n",
            "761:\ttotal: 25m 26s\tremaining: 7m 56s\n",
            "762:\ttotal: 25m 27s\tremaining: 7m 54s\n",
            "763:\ttotal: 25m 30s\tremaining: 7m 52s\n",
            "764:\ttotal: 25m 31s\tremaining: 7m 50s\n",
            "765:\ttest: 0.7885525\tbest: 0.7885525 (765)\ttotal: 25m 34s\tremaining: 7m 48s\n",
            "766:\ttotal: 25m 36s\tremaining: 7m 46s\n",
            "767:\ttotal: 25m 38s\tremaining: 7m 44s\n",
            "768:\ttotal: 25m 40s\tremaining: 7m 42s\n",
            "769:\ttotal: 25m 42s\tremaining: 7m 40s\n",
            "770:\ttest: 0.7885982\tbest: 0.7885982 (770)\ttotal: 25m 44s\tremaining: 7m 38s\n",
            "771:\ttotal: 25m 45s\tremaining: 7m 36s\n",
            "772:\ttotal: 25m 48s\tremaining: 7m 34s\n",
            "773:\ttotal: 25m 49s\tremaining: 7m 32s\n",
            "774:\ttotal: 25m 52s\tremaining: 7m 30s\n",
            "775:\ttest: 0.7886463\tbest: 0.7886463 (775)\ttotal: 25m 53s\tremaining: 7m 28s\n",
            "776:\ttotal: 25m 55s\tremaining: 7m 26s\n",
            "777:\ttotal: 25m 57s\tremaining: 7m 24s\n",
            "778:\ttotal: 25m 58s\tremaining: 7m 22s\n",
            "779:\ttotal: 26m\tremaining: 7m 20s\n",
            "780:\ttest: 0.7886775\tbest: 0.7886775 (780)\ttotal: 26m 3s\tremaining: 7m 18s\n",
            "781:\ttotal: 26m 5s\tremaining: 7m 16s\n",
            "782:\ttotal: 26m 7s\tremaining: 7m 14s\n",
            "783:\ttotal: 26m 9s\tremaining: 7m 12s\n",
            "784:\ttotal: 26m 11s\tremaining: 7m 10s\n",
            "785:\ttest: 0.7887182\tbest: 0.7887182 (785)\ttotal: 26m 12s\tremaining: 7m 8s\n",
            "786:\ttotal: 26m 14s\tremaining: 7m 6s\n",
            "787:\ttotal: 26m 16s\tremaining: 7m 4s\n",
            "788:\ttotal: 26m 18s\tremaining: 7m 2s\n",
            "789:\ttotal: 26m 20s\tremaining: 7m\n",
            "790:\ttest: 0.7887642\tbest: 0.7887642 (790)\ttotal: 26m 22s\tremaining: 6m 58s\n",
            "791:\ttotal: 26m 25s\tremaining: 6m 56s\n",
            "792:\ttotal: 26m 26s\tremaining: 6m 54s\n",
            "793:\ttotal: 26m 28s\tremaining: 6m 52s\n",
            "794:\ttotal: 26m 31s\tremaining: 6m 50s\n",
            "795:\ttest: 0.7888098\tbest: 0.7888098 (795)\ttotal: 26m 32s\tremaining: 6m 48s\n",
            "796:\ttotal: 26m 34s\tremaining: 6m 46s\n",
            "797:\ttotal: 26m 36s\tremaining: 6m 44s\n",
            "798:\ttotal: 26m 38s\tremaining: 6m 42s\n",
            "799:\ttotal: 26m 40s\tremaining: 6m 40s\n",
            "800:\ttest: 0.7888510\tbest: 0.7888510 (800)\ttotal: 26m 42s\tremaining: 6m 38s\n",
            "801:\ttotal: 26m 44s\tremaining: 6m 36s\n",
            "802:\ttotal: 26m 45s\tremaining: 6m 33s\n",
            "803:\ttotal: 26m 48s\tremaining: 6m 32s\n",
            "804:\ttotal: 26m 50s\tremaining: 6m 30s\n",
            "805:\ttest: 0.7888758\tbest: 0.7888758 (805)\ttotal: 26m 52s\tremaining: 6m 28s\n",
            "806:\ttotal: 26m 54s\tremaining: 6m 26s\n",
            "807:\ttotal: 26m 56s\tremaining: 6m 24s\n",
            "808:\ttotal: 26m 57s\tremaining: 6m 21s\n",
            "809:\ttotal: 27m\tremaining: 6m 20s\n",
            "810:\ttest: 0.7889335\tbest: 0.7889335 (810)\ttotal: 27m 2s\tremaining: 6m 18s\n",
            "811:\ttotal: 27m 4s\tremaining: 6m 16s\n",
            "812:\ttotal: 27m 5s\tremaining: 6m 13s\n",
            "813:\ttotal: 27m 7s\tremaining: 6m 11s\n",
            "814:\ttotal: 27m 9s\tremaining: 6m 9s\n",
            "815:\ttest: 0.7889721\tbest: 0.7889721 (815)\ttotal: 27m 12s\tremaining: 6m 8s\n",
            "816:\ttotal: 27m 14s\tremaining: 6m 6s\n",
            "817:\ttotal: 27m 15s\tremaining: 6m 3s\n",
            "818:\ttotal: 27m 17s\tremaining: 6m 1s\n",
            "819:\ttotal: 27m 19s\tremaining: 5m 59s\n",
            "820:\ttest: 0.7890527\tbest: 0.7890527 (820)\ttotal: 27m 21s\tremaining: 5m 57s\n",
            "821:\ttotal: 27m 23s\tremaining: 5m 55s\n",
            "822:\ttotal: 27m 26s\tremaining: 5m 54s\n",
            "823:\ttotal: 27m 28s\tremaining: 5m 52s\n",
            "824:\ttotal: 27m 29s\tremaining: 5m 49s\n",
            "825:\ttest: 0.7890994\tbest: 0.7890994 (825)\ttotal: 27m 32s\tremaining: 5m 48s\n",
            "826:\ttotal: 27m 34s\tremaining: 5m 46s\n",
            "827:\ttotal: 27m 36s\tremaining: 5m 44s\n",
            "828:\ttotal: 27m 37s\tremaining: 5m 41s\n",
            "829:\ttotal: 27m 39s\tremaining: 5m 39s\n",
            "830:\ttest: 0.7891386\tbest: 0.7891386 (830)\ttotal: 27m 41s\tremaining: 5m 37s\n",
            "831:\ttotal: 27m 43s\tremaining: 5m 35s\n",
            "832:\ttotal: 27m 44s\tremaining: 5m 33s\n",
            "833:\ttotal: 27m 46s\tremaining: 5m 31s\n",
            "834:\ttotal: 27m 48s\tremaining: 5m 29s\n",
            "835:\ttest: 0.7891941\tbest: 0.7891941 (835)\ttotal: 27m 50s\tremaining: 5m 27s\n",
            "836:\ttotal: 27m 52s\tremaining: 5m 25s\n",
            "837:\ttotal: 27m 54s\tremaining: 5m 23s\n",
            "838:\ttotal: 27m 55s\tremaining: 5m 21s\n",
            "839:\ttotal: 27m 58s\tremaining: 5m 19s\n",
            "840:\ttest: 0.7892364\tbest: 0.7892364 (840)\ttotal: 28m\tremaining: 5m 17s\n",
            "841:\ttotal: 28m 2s\tremaining: 5m 15s\n",
            "842:\ttotal: 28m 3s\tremaining: 5m 13s\n",
            "843:\ttotal: 28m 5s\tremaining: 5m 11s\n",
            "844:\ttotal: 28m 8s\tremaining: 5m 9s\n",
            "845:\ttest: 0.7892737\tbest: 0.7892737 (845)\ttotal: 28m 10s\tremaining: 5m 7s\n",
            "846:\ttotal: 28m 12s\tremaining: 5m 5s\n",
            "847:\ttotal: 28m 14s\tremaining: 5m 3s\n",
            "848:\ttotal: 28m 16s\tremaining: 5m 1s\n",
            "849:\ttotal: 28m 18s\tremaining: 4m 59s\n",
            "850:\ttest: 0.7893047\tbest: 0.7893047 (850)\ttotal: 28m 21s\tremaining: 4m 57s\n",
            "851:\ttotal: 28m 23s\tremaining: 4m 55s\n",
            "852:\ttotal: 28m 25s\tremaining: 4m 53s\n",
            "853:\ttotal: 28m 27s\tremaining: 4m 51s\n",
            "854:\ttotal: 28m 28s\tremaining: 4m 49s\n",
            "855:\ttest: 0.7893710\tbest: 0.7893710 (855)\ttotal: 28m 30s\tremaining: 4m 47s\n",
            "856:\ttotal: 28m 32s\tremaining: 4m 45s\n",
            "857:\ttotal: 28m 34s\tremaining: 4m 43s\n",
            "858:\ttotal: 28m 36s\tremaining: 4m 41s\n",
            "859:\ttotal: 28m 38s\tremaining: 4m 39s\n",
            "860:\ttest: 0.7893982\tbest: 0.7893982 (860)\ttotal: 28m 41s\tremaining: 4m 37s\n",
            "861:\ttotal: 28m 43s\tremaining: 4m 35s\n",
            "862:\ttotal: 28m 45s\tremaining: 4m 33s\n",
            "863:\ttotal: 28m 47s\tremaining: 4m 31s\n",
            "864:\ttotal: 28m 49s\tremaining: 4m 29s\n",
            "865:\ttest: 0.7894344\tbest: 0.7894344 (865)\ttotal: 28m 51s\tremaining: 4m 27s\n",
            "866:\ttotal: 28m 53s\tremaining: 4m 25s\n",
            "867:\ttotal: 28m 55s\tremaining: 4m 23s\n",
            "868:\ttotal: 28m 58s\tremaining: 4m 22s\n",
            "869:\ttotal: 28m 59s\tremaining: 4m 19s\n",
            "870:\ttest: 0.7894703\tbest: 0.7894703 (870)\ttotal: 29m 1s\tremaining: 4m 17s\n",
            "871:\ttotal: 29m 3s\tremaining: 4m 15s\n",
            "872:\ttotal: 29m 5s\tremaining: 4m 13s\n",
            "873:\ttotal: 29m 7s\tremaining: 4m 11s\n",
            "874:\ttotal: 29m 10s\tremaining: 4m 10s\n",
            "875:\ttest: 0.7894961\tbest: 0.7894961 (875)\ttotal: 29m 13s\tremaining: 4m 8s\n",
            "876:\ttotal: 29m 14s\tremaining: 4m 6s\n",
            "877:\ttotal: 29m 17s\tremaining: 4m 4s\n",
            "878:\ttotal: 29m 19s\tremaining: 4m 2s\n",
            "879:\ttotal: 29m 21s\tremaining: 4m\n",
            "880:\ttest: 0.7895421\tbest: 0.7895421 (880)\ttotal: 29m 23s\tremaining: 3m 58s\n",
            "881:\ttotal: 29m 25s\tremaining: 3m 56s\n",
            "882:\ttotal: 29m 27s\tremaining: 3m 54s\n",
            "883:\ttotal: 29m 29s\tremaining: 3m 52s\n",
            "884:\ttotal: 29m 30s\tremaining: 3m 50s\n",
            "885:\ttest: 0.7895766\tbest: 0.7895766 (885)\ttotal: 29m 32s\tremaining: 3m 48s\n",
            "886:\ttotal: 29m 34s\tremaining: 3m 46s\n",
            "887:\ttotal: 29m 35s\tremaining: 3m 43s\n",
            "888:\ttotal: 29m 37s\tremaining: 3m 41s\n",
            "889:\ttotal: 29m 40s\tremaining: 3m 40s\n",
            "890:\ttest: 0.7896272\tbest: 0.7896272 (890)\ttotal: 29m 42s\tremaining: 3m 38s\n",
            "891:\ttotal: 29m 44s\tremaining: 3m 36s\n",
            "892:\ttotal: 29m 46s\tremaining: 3m 34s\n",
            "893:\ttotal: 29m 49s\tremaining: 3m 32s\n",
            "894:\ttotal: 29m 50s\tremaining: 3m 30s\n",
            "895:\ttest: 0.7896570\tbest: 0.7896570 (895)\ttotal: 29m 53s\tremaining: 3m 28s\n",
            "896:\ttotal: 29m 54s\tremaining: 3m 26s\n",
            "897:\ttotal: 29m 56s\tremaining: 3m 24s\n",
            "898:\ttotal: 29m 59s\tremaining: 3m 22s\n",
            "899:\ttotal: 30m 2s\tremaining: 3m 20s\n",
            "900:\ttest: 0.7896955\tbest: 0.7896955 (900)\ttotal: 30m 3s\tremaining: 3m 18s\n",
            "901:\ttotal: 30m 5s\tremaining: 3m 16s\n",
            "902:\ttotal: 30m 7s\tremaining: 3m 14s\n",
            "903:\ttotal: 30m 9s\tremaining: 3m 12s\n",
            "904:\ttotal: 30m 11s\tremaining: 3m 10s\n",
            "905:\ttest: 0.7897332\tbest: 0.7897332 (905)\ttotal: 30m 14s\tremaining: 3m 8s\n",
            "906:\ttotal: 30m 16s\tremaining: 3m 6s\n",
            "907:\ttotal: 30m 18s\tremaining: 3m 4s\n",
            "908:\ttotal: 30m 19s\tremaining: 3m 2s\n",
            "909:\ttotal: 30m 22s\tremaining: 3m\n",
            "910:\ttest: 0.7897795\tbest: 0.7897795 (910)\ttotal: 30m 24s\tremaining: 2m 58s\n",
            "911:\ttotal: 30m 26s\tremaining: 2m 56s\n",
            "912:\ttotal: 30m 28s\tremaining: 2m 54s\n",
            "913:\ttotal: 30m 30s\tremaining: 2m 52s\n",
            "914:\ttotal: 30m 32s\tremaining: 2m 50s\n",
            "915:\ttest: 0.7898398\tbest: 0.7898398 (915)\ttotal: 30m 34s\tremaining: 2m 48s\n",
            "916:\ttotal: 30m 36s\tremaining: 2m 46s\n",
            "917:\ttotal: 30m 38s\tremaining: 2m 44s\n",
            "918:\ttotal: 30m 40s\tremaining: 2m 42s\n",
            "919:\ttotal: 30m 42s\tremaining: 2m 40s\n",
            "920:\ttest: 0.7898812\tbest: 0.7898812 (920)\ttotal: 30m 44s\tremaining: 2m 38s\n",
            "921:\ttotal: 30m 46s\tremaining: 2m 36s\n",
            "922:\ttotal: 30m 48s\tremaining: 2m 34s\n",
            "923:\ttotal: 30m 51s\tremaining: 2m 32s\n",
            "924:\ttotal: 30m 52s\tremaining: 2m 30s\n",
            "925:\ttest: 0.7899290\tbest: 0.7899290 (925)\ttotal: 30m 54s\tremaining: 2m 28s\n",
            "926:\ttotal: 30m 56s\tremaining: 2m 26s\n",
            "927:\ttotal: 30m 59s\tremaining: 2m 24s\n",
            "928:\ttotal: 31m 1s\tremaining: 2m 22s\n",
            "929:\ttotal: 31m 3s\tremaining: 2m 20s\n",
            "930:\ttest: 0.7899582\tbest: 0.7899582 (930)\ttotal: 31m 5s\tremaining: 2m 18s\n",
            "931:\ttotal: 31m 7s\tremaining: 2m 16s\n",
            "932:\ttotal: 31m 9s\tremaining: 2m 14s\n",
            "933:\ttotal: 31m 12s\tremaining: 2m 12s\n",
            "934:\ttotal: 31m 14s\tremaining: 2m 10s\n",
            "935:\ttest: 0.7899875\tbest: 0.7899875 (935)\ttotal: 31m 16s\tremaining: 2m 8s\n",
            "936:\ttotal: 31m 17s\tremaining: 2m 6s\n",
            "937:\ttotal: 31m 19s\tremaining: 2m 4s\n",
            "938:\ttotal: 31m 21s\tremaining: 2m 2s\n",
            "939:\ttotal: 31m 24s\tremaining: 2m\n",
            "940:\ttest: 0.7900243\tbest: 0.7900243 (940)\ttotal: 31m 26s\tremaining: 1m 58s\n",
            "941:\ttotal: 31m 28s\tremaining: 1m 56s\n",
            "942:\ttotal: 31m 30s\tremaining: 1m 54s\n",
            "943:\ttotal: 31m 32s\tremaining: 1m 52s\n",
            "944:\ttotal: 31m 34s\tremaining: 1m 50s\n",
            "945:\ttest: 0.7900667\tbest: 0.7900667 (945)\ttotal: 31m 36s\tremaining: 1m 48s\n",
            "946:\ttotal: 31m 38s\tremaining: 1m 46s\n",
            "947:\ttotal: 31m 40s\tremaining: 1m 44s\n",
            "948:\ttotal: 31m 42s\tremaining: 1m 42s\n",
            "949:\ttotal: 31m 44s\tremaining: 1m 40s\n",
            "950:\ttest: 0.7900965\tbest: 0.7900965 (950)\ttotal: 31m 47s\tremaining: 1m 38s\n",
            "951:\ttotal: 31m 49s\tremaining: 1m 36s\n",
            "952:\ttotal: 31m 51s\tremaining: 1m 34s\n",
            "953:\ttotal: 31m 53s\tremaining: 1m 32s\n",
            "954:\ttotal: 31m 55s\tremaining: 1m 30s\n",
            "955:\ttest: 0.7901372\tbest: 0.7901372 (955)\ttotal: 31m 57s\tremaining: 1m 28s\n",
            "956:\ttotal: 31m 59s\tremaining: 1m 26s\n",
            "957:\ttotal: 32m 1s\tremaining: 1m 24s\n",
            "958:\ttotal: 32m 3s\tremaining: 1m 22s\n",
            "959:\ttotal: 32m 5s\tremaining: 1m 20s\n",
            "960:\ttest: 0.7901893\tbest: 0.7901893 (960)\ttotal: 32m 7s\tremaining: 1m 18s\n",
            "961:\ttotal: 32m 9s\tremaining: 1m 16s\n",
            "962:\ttotal: 32m 11s\tremaining: 1m 14s\n",
            "963:\ttotal: 32m 13s\tremaining: 1m 12s\n",
            "964:\ttotal: 32m 16s\tremaining: 1m 10s\n",
            "965:\ttest: 0.7902194\tbest: 0.7902194 (965)\ttotal: 32m 18s\tremaining: 1m 8s\n",
            "966:\ttotal: 32m 19s\tremaining: 1m 6s\n",
            "967:\ttotal: 32m 21s\tremaining: 1m 4s\n",
            "968:\ttotal: 32m 23s\tremaining: 1m 2s\n",
            "969:\ttotal: 32m 25s\tremaining: 1m\n",
            "970:\ttest: 0.7902470\tbest: 0.7902470 (970)\ttotal: 32m 27s\tremaining: 58.2s\n",
            "971:\ttotal: 32m 30s\tremaining: 56.2s\n",
            "972:\ttotal: 32m 31s\tremaining: 54.2s\n",
            "973:\ttotal: 32m 33s\tremaining: 52.2s\n",
            "974:\ttotal: 32m 35s\tremaining: 50.1s\n",
            "975:\ttest: 0.7902847\tbest: 0.7902847 (975)\ttotal: 32m 37s\tremaining: 48.1s\n",
            "976:\ttotal: 32m 39s\tremaining: 46.1s\n",
            "977:\ttotal: 32m 41s\tremaining: 44.1s\n",
            "978:\ttotal: 32m 42s\tremaining: 42.1s\n",
            "979:\ttotal: 32m 44s\tremaining: 40.1s\n",
            "980:\ttest: 0.7903208\tbest: 0.7903208 (980)\ttotal: 32m 46s\tremaining: 38.1s\n",
            "981:\ttotal: 32m 48s\tremaining: 36.1s\n",
            "982:\ttotal: 32m 50s\tremaining: 34.1s\n",
            "983:\ttotal: 32m 52s\tremaining: 32.1s\n",
            "984:\ttotal: 32m 53s\tremaining: 30.1s\n",
            "985:\ttest: 0.7903446\tbest: 0.7903446 (985)\ttotal: 32m 55s\tremaining: 28.1s\n",
            "986:\ttotal: 32m 58s\tremaining: 26.1s\n",
            "987:\ttotal: 33m\tremaining: 24.1s\n",
            "988:\ttotal: 33m 2s\tremaining: 22.1s\n",
            "989:\ttotal: 33m 4s\tremaining: 20s\n",
            "990:\ttest: 0.7903763\tbest: 0.7903763 (990)\ttotal: 33m 7s\tremaining: 18s\n",
            "991:\ttotal: 33m 9s\tremaining: 16s\n",
            "992:\ttotal: 33m 11s\tremaining: 14s\n",
            "993:\ttotal: 33m 13s\tremaining: 12s\n",
            "994:\ttotal: 33m 15s\tremaining: 10s\n",
            "995:\ttest: 0.7904189\tbest: 0.7904189 (995)\ttotal: 33m 17s\tremaining: 8.02s\n",
            "996:\ttotal: 33m 19s\tremaining: 6.02s\n",
            "997:\ttotal: 33m 20s\tremaining: 4.01s\n",
            "998:\ttotal: 33m 22s\tremaining: 2s\n",
            "999:\ttest: 0.7904423\tbest: 0.7904423 (999)\ttotal: 33m 25s\tremaining: 0us\n",
            "bestTest = 0.7904423177\n",
            "bestIteration = 999\n"
          ]
        }
      ],
      "source": [
        "X_train = train_df.drop('label').to_pandas()\n",
        "y_train = train_df['label'].to_pandas()\n",
        "X_test = test_df.drop('label').to_pandas()\n",
        "y_test = test_df['label'].to_pandas()\n",
        "\n",
        "train_pool = Pool(X_train, y_train, cat_features=CriteoDatasetUtils.CAT_COLS)\n",
        "test_pool = Pool(X_test, y_test, cat_features=CriteoDatasetUtils.CAT_COLS)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    early_stopping_rounds=50,\n",
        "    task_type=\"GPU\"\n",
        ")\n",
        "model.fit(train_pool, eval_set=test_pool, use_best_model=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP4KpqJSSC9R"
      },
      "source": [
        "From [DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/abs/2008.13535):\n",
        "<div style=\"width:50%; margin: auto;\">\n",
        "\n",
        "![](https://i.ibb.co/HDHJ8Nzq/level-improvement.png)\n",
        "![](https://i.ibb.co/fYpyrKBs/table.png)\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}